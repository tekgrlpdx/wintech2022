2020-08-09 20:25:44,908 sagemaker-containers INFO     Imported framework sagemaker_bootstrap
2020-08-09 20:25:44,909 sagemaker_bootstrap INFO     SM_USER_ARGS=["--aws_region","us-east-1","--batch_size","64","--beta_entropy","0.01","--discount_factor","0.99","--e_greedy_value","1","--epsilon_steps","10000","--exploration_type","Categorical","--loss_type","Huber","--lr","0.0001","--model_metadata_s3_key","s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/model_metadata.json","--num_episodes_between_training","40","--num_epochs","10","--pretrained_s3_bucket","aws-deepracer-data-us-east-1-1","--pretrained_s3_prefix","data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa/sagemaker-robomaker-artifacts","--reward_function_s3_source","s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/reward_function.py","--s3_bucket","aws-deepracer-data-us-east-1-1","--s3_kms_cmk_arn","arn:aws:kms:us-east-1:372026249783:key/b76b771b-5b6d-4b0d-a01d-ec558153c4d4","--s3_prefix","data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts","--stack_size","1"]
2020-08-09 20:25:44,910 sagemaker_bootstrap INFO     All eniron vars=environ({'PATH': '/opt/ml/code/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_HP_PRETRAINED_S3_PREFIX': 'data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa/sagemaker-robomaker-artifacts', 'SM_HP_MODEL_METADATA_S3_KEY': 's3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/model_metadata.json', 'SM_HP_BATCH_SIZE': '64', 'SAGEMAKER_JOB_NAME': '', 'SM_HP_DISCOUNT_FACTOR': '0.99', 'SM_HP_AWS_REGION': 'us-east-1', 'SM_CURRENT_HOST': 'algo-1', 'NVIDIA_VISIBLE_DEVICES': 'void', 'SM_HP_EXPLORATION_TYPE': 'Categorical', 'SM_HP_S3_PREFIX': 'data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_CHANNELS': '[]', 'AWS_REGION': 'us-east-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_LOG_LEVEL': '20', 'SM_HP_BETA_ENTROPY': '0.01', 'SAGEMAKER_TRAINING_COMMAND': '/opt/ml/code/sage-train.sh', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/678889e3-8a47-45b9-b197-ab955da1244f', 'SAGEMAKER_REGION': '', 'SM_FRAMEWORK_MODULE': 'sagemaker_bootstrap:train', 'SM_MODULE_NAME': '', 'SM_HP_LR': '0.0001', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/ae16efe7-89fd-4002-8126-c93642fd60f2', 'HOME': '/root', 'SM_MODULE_DIR': '', 'HOSTNAME': 'ip-10-0-77-228.ec2.internal', 'SM_HP_EPSILON_STEPS': '10000', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/ae16efe7-89fd-4002-8126-c93642fd60f2', 'SM_HP_E_GREEDY_VALUE': '1', 'SM_HP_REWARD_FUNCTION_S3_SOURCE': 's3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/reward_function.py', 'SM_NUM_CPUS': '8', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{"aws_region":"us-east-1","batch_size":64,"beta_entropy":0.01,"discount_factor":0.99,"e_greedy_value":1,"epsilon_steps":10000,"exploration_type":"Categorical","loss_type":"Huber","lr":0.0001,"model_metadata_s3_key":"s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/model_metadata.json","num_episodes_between_training":40,"num_epochs":10,"pretrained_s3_bucket":"aws-deepracer-data-us-east-1-1","pretrained_s3_prefix":"data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa/sagemaker-robomaker-artifacts","reward_function_s3_source":"s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/reward_function.py","s3_bucket":"aws-deepracer-data-us-east-1-1","s3_kms_cmk_arn":"arn:aws:kms:us-east-1:372026249783:key/b76b771b-5b6d-4b0d-a01d-ec558153c4d4","s3_prefix":"data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts","stack_size":1}', 'SM_HP_S3_BUCKET': 'aws-deepracer-data-us-east-1-1', 'PYTHONUNBUFFERED': '1', 'SM_HP_NUM_EPISODES_BETWEEN_TRAINING': '40', 'SM_HOSTS': '["algo-1"]', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:372026249783:training-job/dr-sm-rltj--20200809202255-59a0e273-3d7b-4610-b310-7d18256f55e3', 'NODE_TYPE': 'SAGEMAKER_TRAINING_WORKER', 'SM_TRAINING_ENV': '{"channel_input_dirs":{},"current_host":"algo-1","framework_module":"sagemaker_bootstrap:train","hosts":["algo-1"],"hyperparameters":{"aws_region":"us-east-1","batch_size":64,"beta_entropy":0.01,"discount_factor":0.99,"e_greedy_value":1,"epsilon_steps":10000,"exploration_type":"Categorical","loss_type":"Huber","lr":0.0001,"model_metadata_s3_key":"s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/model_metadata.json","num_episodes_between_training":40,"num_epochs":10,"pretrained_s3_bucket":"aws-deepracer-data-us-east-1-1","pretrained_s3_prefix":"data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa/sagemaker-robomaker-artifacts","reward_function_s3_source":"s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/reward_function.py","s3_bucket":"aws-deepracer-data-us-east-1-1","s3_kms_cmk_arn":"arn:aws:kms:us-east-1:372026249783:key/b76b771b-5b6d-4b0d-a01d-ec558153c4d4","s3_prefix":"data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts","stack_size":1},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","job_name":"dr-sm-rltj--20200809202255-59a0e273-3d7b-4610-b310-7d18256f55e3","log_level":20,"model_dir":"/opt/ml/model","module_dir":"","module_name":"","network_interface_name":"eth0","num_cpus":8,"num_gpus":0,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","resource_config":{"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"}}', 'TRAINING_JOB_NAME': 'dr-sm-rltj--20200809202255-59a0e273-3d7b-4610-b310-7d18256f55e3', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'SM_HP_S3_KMS_CMK_ARN': 'arn:aws:kms:us-east-1:372026249783:key/b76b771b-5b6d-4b0d-a01d-ec558153c4d4', 'DMLC_INTERFACE': 'eth0', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_HP_PRETRAINED_S3_BUCKET': 'aws-deepracer-data-us-east-1-1', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_bootstrap:train', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_USER_ARGS': '["--aws_region","us-east-1","--batch_size","64","--beta_entropy","0.01","--discount_factor","0.99","--e_greedy_value","1","--epsilon_steps","10000","--exploration_type","Categorical","--loss_type","Huber","--lr","0.0001","--model_metadata_s3_key","s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/model_metadata.json","--num_episodes_between_training","40","--num_epochs","10","--pretrained_s3_bucket","aws-deepracer-data-us-east-1-1","--pretrained_s3_prefix","data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa/sagemaker-robomaker-artifacts","--reward_function_s3_source","s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/reward_function.py","--s3_bucket","aws-deepracer-data-us-east-1-1","--s3_kms_cmk_arn","arn:aws:kms:us-east-1:372026249783:key/b76b771b-5b6d-4b0d-a01d-ec558153c4d4","--s3_prefix","data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts","--stack_size","1"]', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_NUM_GPUS': '0', 'SM_HP_STACK_SIZE': '1', 'SM_HP_NUM_EPOCHS': '10', 'SM_RESOURCE_CONFIG': '{"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"}', 'PYTHONPATH': '/opt/ml/code/:/opt/amazon/:', 'SM_HP_LOSS_TYPE': 'Huber', 'CURRENT_HOST': 'algo-1'})
2020-08-09 20:25:44,910 sagemaker_bootstrap INFO     Launching training command: /opt/ml/code/sage-train.sh --aws_region us-east-1 --batch_size 64 --beta_entropy 0.01 --discount_factor 0.99 --e_greedy_value 1 --epsilon_steps 10000 --exploration_type Categorical --loss_type Huber --lr 0.0001 --model_metadata_s3_key s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/model_metadata.json --num_episodes_between_training 40 --num_epochs 10 --pretrained_s3_bucket aws-deepracer-data-us-east-1-1 --pretrained_s3_prefix data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa/sagemaker-robomaker-artifacts --reward_function_s3_source s3://aws-deepracer-data-us-east-1-1/data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/reward_function.py --s3_bucket aws-deepracer-data-us-east-1-1 --s3_kms_cmk_arn arn:aws:kms:us-east-1:372026249783:key/b76b771b-5b6d-4b0d-a01d-ec558153c4d4 --s3_prefix data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts --stack_size 1
Starting sage-train.sh
18:C 09 Aug 2020 20:25:44.950 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
18:C 09 Aug 2020 20:25:44.950 # Redis version=6.0.6, bits=64, commit=00000000, modified=0, pid=18, just started
18:C 09 Aug 2020 20:25:44.950 # Configuration loaded
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 6.0.6 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                   
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 18
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           http://redis.io        
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               

18:M 09 Aug 2020 20:25:44.951 # WARNING: The TCP backlog setting of 512 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
18:M 09 Aug 2020 20:25:44.951 # Server initialized
18:M 09 Aug 2020 20:25:44.951 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
18:M 09 Aug 2020 20:25:44.951 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
18:M 09 Aug 2020 20:25:44.951 * Ready to accept connections
Initializing SageS3Client...
[s3] Successfully downloaded model metadata                  from s3 key data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/model_metadata.json to local ./custom_files/agent/model_metadata.json.
Sensor list ['LIDAR', 'STEREO_CAMERAS'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Loaded action space from file: [{'steering_angle': -30, 'index': 0, 'speed': 0.6}, {'steering_angle': -30, 'index': 1, 'speed': 1.2}, {'steering_angle': -15, 'index': 2, 'speed': 0.6}, {'steering_angle': -15, 'index': 3, 'speed': 1.2}, {'steering_angle': 0, 'index': 4, 'speed': 0.6}, {'steering_angle': 0, 'index': 5, 'speed': 1.2}, {'steering_angle': 15, 'index': 6, 'speed': 0.6}, {'steering_angle': 15, 'index': 7, 'speed': 1.2}, {'steering_angle': 30, 'index': 8, 'speed': 0.6}, {'steering_angle': 30, 'index': 9, 'speed': 1.2}]
Using the following hyper-parameters
{
  "batch_size": 64,
  "beta_entropy": 0.01,
  "discount_factor": 0.99,
  "e_greedy_value": 1.0,
  "epsilon_steps": 10000,
  "exploration_type": "categorical",
  "loss_type": "huber",
  "lr": 0.0001,
  "num_episodes_between_training": 40,
  "num_epochs": 10,
  "stack_size": 1,
  "term_cond_avg_score": 100000.0,
  "term_cond_max_episodes": 100000
}
[s3] Successfully upload hyperparameters to                  s3 bucket aws-deepracer-data-us-east-1-1 with s3 key data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/ip/hyperparameters.json.
Hostname: ip-10-0-77-228.ec2.internal
[s3] Successfully upload ip address to                  s3 bucket aws-deepracer-data-us-east-1-1 with s3 key data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/ip/ip.json.
[s3] Successfully upload ip done to                  s3 bucket aws-deepracer-data-us-east-1-1 with s3 key data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/ip/done.
Unable to find best model data, using last model
Exit model selection because model_checkpoint is None of type <class 'NoneType'>
## Creating graph - name: MultiAgentGraphManager
## Start physics before creating graph
## Create graph
## Creating agent - name: agent
[RL] Created agent loggers
[RL] Dynamic import of memory:  "DeepRacerMemoryParameters" {
    "load_memory_from_file_path": null,
    "max_size": [
        "<MemoryGranularity.Transitions: 0>",
        1000000
    ],
    "n_step": -1,
    "shared_memory": false,
    "train_to_eval_ratio": 1
}

[RL] Dynamically imported of memory <markov.memories.deepracer_memory.DeepRacerMemory object at 0x7f6227bb7b38>
[RL] Setting devices
[RL] Setting filters
[RL] Setting filter devices: numpy
[RL] Setting Phase
[RL] After setting Phase
[RL] Setting signals
[RL] Agent init successful
[RL] ActorCriticAgent init
[RL] ActorCriticAgent  init successful
## Created agent: agent
## Stop physics after creating graph
## Creating session
2020-08-09 20:26:34.149834: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
Checkpoint> Restoring from path=./pretrained_checkpoint/89_Step-37062.ckpt
INFO:tensorflow:./checkpoint/90_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/90_Step-0.ckpt']
Uploaded 3 files for checkpoint 90
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_90.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_90.pb to /opt/ml/model/agent/model.pb.
Uploaded 3 files for checkpoint 90
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_90.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_90.pb to /opt/ml/model/agent/model.pb.
DoorMan: installing SIGINT, SIGTERM
Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=0, Steps=41, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=0, Steps=66, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=0, Steps=96, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=0, Steps=106, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=0, Steps=150, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=0, Steps=172, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=0, Steps=190, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=0, Steps=237, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=0, Steps=262, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=0, Steps=276, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=0, Steps=313, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=0, Steps=323, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=0, Steps=350, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=0, Steps=361, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=0, Steps=362, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=0, Steps=376, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=0, Steps=394, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=0, Steps=460, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=0, Steps=469, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=0, Steps=503, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=0, Steps=530, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=0, Steps=585, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=0, Steps=622, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=0, Steps=629, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=0, Steps=668, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=0, Steps=683, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=0, Steps=698, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=0, Steps=764, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=0, Steps=800, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=0, Steps=819, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=0, Steps=833, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=0, Steps=844, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=0, Steps=876, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=0, Steps=881, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=0, Steps=902, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=0, Steps=929, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=0, Steps=958, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=0, Steps=968, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=0, Steps=984, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=0, Steps=1036, Training iteration=0
Policy training> Surrogate loss=0.003961808048188686, KL divergence=0.0018533687107264996, Entropy=1.9144972562789917, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.05664987862110138, KL divergence=0.004129825625568628, Entropy=1.892281174659729, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.09028918296098709, KL divergence=0.006228366866707802, Entropy=1.8569278717041016, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.11414187401533127, KL divergence=0.0071698641404509544, Entropy=1.8672763109207153, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1261168122291565, KL divergence=0.00912995170801878, Entropy=1.8586465120315552, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13418984413146973, KL divergence=0.01054953970015049, Entropy=1.861504077911377, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13175490498542786, KL divergence=0.011837187223136425, Entropy=1.8660671710968018, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13418810069561005, KL divergence=0.013034800998866558, Entropy=1.8593348264694214, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13789452612400055, KL divergence=0.013767851516604424, Entropy=1.8665008544921875, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14491480588912964, KL divergence=0.01429213397204876, Entropy=1.8650729656219482, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/91_Step-1036.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/91_Step-1036.ckpt']
Uploaded 3 files for checkpoint 91
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_91.pb
Best checkpoint number: 90, Last checkpoint number: 90
Copying the frozen checkpoint from ./frozen_models/agent/model_90.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=0, Steps=1074, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=0, Steps=1121, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=0, Steps=1141, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=0, Steps=1169, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=0, Steps=1183, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=0, Steps=1240, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=0, Steps=1256, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=0, Steps=1280, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=0, Steps=1314, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=0, Steps=1340, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=0, Steps=1353, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=0, Steps=1384, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=0, Steps=1416, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=0, Steps=1464, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=0, Steps=1473, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=0, Steps=1488, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=0, Steps=1517, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=0, Steps=1572, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=0, Steps=1600, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=0, Steps=1654, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=0, Steps=1685, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=0, Steps=1713, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=0, Steps=1751, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=0, Steps=1776, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=0, Steps=1814, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=0, Steps=1828, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=0, Steps=1841, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=0, Steps=1867, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=0, Steps=1881, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=0, Steps=1882, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=0, Steps=1938, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=0, Steps=1949, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=0, Steps=1961, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=0, Steps=1976, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=0, Steps=2012, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=0, Steps=2020, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=0, Steps=2061, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=0, Steps=2080, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=0, Steps=2090, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=0, Steps=2128, Training iteration=1
Policy training> Surrogate loss=-0.006866383366286755, KL divergence=0.0034095158334821463, Entropy=1.8384222984313965, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.08262472599744797, KL divergence=0.011135096661746502, Entropy=1.8033339977264404, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11070650070905685, KL divergence=0.010931206867098808, Entropy=1.8334428071975708, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12138676643371582, KL divergence=0.013200427405536175, Entropy=1.8319069147109985, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.12486401945352554, KL divergence=0.014268304221332073, Entropy=1.8321677446365356, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.134074866771698, KL divergence=0.015487702563405037, Entropy=1.8276666402816772, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1379595845937729, KL divergence=0.01620272547006607, Entropy=1.8268836736679077, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13772757351398468, KL divergence=0.01654740422964096, Entropy=1.8320691585540771, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13628114759922028, KL divergence=0.017215274274349213, Entropy=1.834009051322937, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14116865396499634, KL divergence=0.01795150712132454, Entropy=1.8301336765289307, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/92_Step-2128.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/92_Step-2128.ckpt']
Uploaded 3 files for checkpoint 92
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_92.pb
Best checkpoint number: 90, Last checkpoint number: 90
Copying the frozen checkpoint from ./frozen_models/agent/model_90.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=0, Steps=2150, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=0, Steps=2161, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=0, Steps=2190, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=0, Steps=2200, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=0, Steps=2210, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=0, Steps=2224, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=0, Steps=2271, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=0, Steps=2293, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=0, Steps=2313, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=0, Steps=2322, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=0, Steps=2358, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=0, Steps=2376, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=0, Steps=2400, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=0, Steps=2407, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=0, Steps=2417, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=0, Steps=2454, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=0, Steps=2469, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=0, Steps=2479, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=0, Steps=2484, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=0, Steps=2523, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=0, Steps=2552, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=0, Steps=2564, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=0, Steps=2578, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=0, Steps=2595, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=0, Steps=2603, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=0, Steps=2622, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=0, Steps=2629, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=0, Steps=2700, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=0, Steps=2719, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=0, Steps=2777, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=0, Steps=2790, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=0, Steps=2895, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=0, Steps=2915, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=0, Steps=2927, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=0, Steps=2943, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=0, Steps=3073, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=0, Steps=3085, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=0, Steps=3097, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=0, Steps=3173, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=0, Steps=3205, Training iteration=2
Policy training> Surrogate loss=0.00412505678832531, KL divergence=0.002944084582850337, Entropy=1.7842576503753662, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09811621159315109, KL divergence=0.012575612403452396, Entropy=1.7720221281051636, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11736242473125458, KL divergence=0.013567269779741764, Entropy=1.8040971755981445, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13825388252735138, KL divergence=0.016751589253544807, Entropy=1.7946288585662842, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.137089341878891, KL divergence=0.01816461980342865, Entropy=1.8082282543182373, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.14080701768398285, KL divergence=0.02042209543287754, Entropy=1.8056161403656006, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13528667390346527, KL divergence=0.021362673491239548, Entropy=1.8143129348754883, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14252524077892303, KL divergence=0.021839970722794533, Entropy=1.8165926933288574, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.1437681019306183, KL divergence=0.022024139761924744, Entropy=1.8179240226745605, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14959093928337097, KL divergence=0.02289525419473648, Entropy=1.8257887363433838, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/93_Step-3205.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/93_Step-3205.ckpt']
Uploaded 3 files for checkpoint 93
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_93.pb
Best checkpoint number: 90, Last checkpoint number: 91
Copying the frozen checkpoint from ./frozen_models/agent/model_90.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'91'}
Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=0, Steps=3234, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=0, Steps=3259, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=0, Steps=3271, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=0, Steps=3304, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=0, Steps=3318, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=0, Steps=3345, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=0, Steps=3352, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=0, Steps=3374, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=0, Steps=3406, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=0, Steps=3420, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=0, Steps=3435, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=0, Steps=3447, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=0, Steps=3481, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=0, Steps=3490, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=0, Steps=3503, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=0, Steps=3527, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=0, Steps=3560, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=0, Steps=3571, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=0, Steps=3575, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=0, Steps=3661, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=0, Steps=3709, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=0, Steps=3722, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=0, Steps=3740, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=0, Steps=3766, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=0, Steps=3781, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=0, Steps=3804, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=0, Steps=3820, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=0, Steps=3862, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=0, Steps=3879, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=0, Steps=3891, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=0, Steps=3905, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=0, Steps=3922, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=0, Steps=3944, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=0, Steps=3961, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=0, Steps=3980, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=0, Steps=3983, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=0, Steps=3993, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=0, Steps=4022, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=0, Steps=4071, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=0, Steps=4205, Training iteration=3
Policy training> Surrogate loss=0.009845588356256485, KL divergence=0.002153614768758416, Entropy=1.848726749420166, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.08865196257829666, KL divergence=0.008893903344869614, Entropy=1.818546175956726, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11287302523851395, KL divergence=0.012977784499526024, Entropy=1.8213307857513428, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.1296217143535614, KL divergence=0.015538886189460754, Entropy=1.8262711763381958, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1353905349969864, KL divergence=0.018304772675037384, Entropy=1.8182246685028076, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13120980560779572, KL divergence=0.019046524539589882, Entropy=1.8227944374084473, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1429249495267868, KL divergence=0.019675619900226593, Entropy=1.8233948945999146, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13608364760875702, KL divergence=0.020820047706365585, Entropy=1.8135050535202026, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.1356455683708191, KL divergence=0.02053373120725155, Entropy=1.8232877254486084, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.12587980926036835, KL divergence=0.021010754629969597, Entropy=1.8201587200164795, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/94_Step-4205.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/94_Step-4205.ckpt']
Uploaded 3 files for checkpoint 94
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_94.pb
Best checkpoint number: 90, Last checkpoint number: 92
Copying the frozen checkpoint from ./frozen_models/agent/model_90.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'92'}
Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=0, Steps=4239, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=0, Steps=4253, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=0, Steps=4268, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=0, Steps=4310, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=0, Steps=4329, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=0, Steps=4363, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=0, Steps=4376, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=0, Steps=4417, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=0, Steps=4427, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=0, Steps=4472, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=0, Steps=4499, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=0, Steps=4537, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=0, Steps=4564, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=0, Steps=4581, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=0, Steps=4632, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=0, Steps=4685, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=0, Steps=4712, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=0, Steps=4733, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=0, Steps=4797, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=0, Steps=4838, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=0, Steps=4860, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=0, Steps=4882, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=0, Steps=4914, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=0, Steps=4997, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=0, Steps=5021, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=0, Steps=5054, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=0, Steps=5061, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=0, Steps=5093, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=0, Steps=5105, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=0, Steps=5119, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=0, Steps=5128, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=0, Steps=5178, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=0, Steps=5203, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=0, Steps=5229, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=0, Steps=5243, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=0, Steps=5273, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=0, Steps=5285, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=0, Steps=5295, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=0, Steps=5308, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=0, Steps=5356, Training iteration=4
Policy training> Surrogate loss=-0.0036216084845364094, KL divergence=0.004322476219385862, Entropy=1.785868763923645, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.08057315647602081, KL divergence=0.01655559241771698, Entropy=1.7605019807815552, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12136436998844147, KL divergence=0.018827276304364204, Entropy=1.7581039667129517, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12295942008495331, KL divergence=0.02122836373746395, Entropy=1.7529383897781372, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13344262540340424, KL divergence=0.022804271429777145, Entropy=1.7518398761749268, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1202348843216896, KL divergence=0.023285193368792534, Entropy=1.7554924488067627, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1388794183731079, KL divergence=0.02345122955739498, Entropy=1.762993335723877, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14544284343719482, KL divergence=0.024611659348011017, Entropy=1.767486810684204, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13803011178970337, KL divergence=0.024283528327941895, Entropy=1.7715734243392944, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.13561277091503143, KL divergence=0.02491776831448078, Entropy=1.7728255987167358, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/95_Step-5356.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/95_Step-5356.ckpt']
Uploaded 3 files for checkpoint 95
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_95.pb
Best checkpoint number: 90, Last checkpoint number: 93
Copying the frozen checkpoint from ./frozen_models/agent/model_90.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'93'}
Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=0, Steps=5411, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=0, Steps=5431, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=0, Steps=5489, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=0, Steps=5506, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=0, Steps=5519, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=0, Steps=5541, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=0, Steps=5553, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=0, Steps=5570, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=0, Steps=5582, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=0, Steps=5626, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=0, Steps=5647, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=0, Steps=5678, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=0, Steps=5740, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=0, Steps=5753, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=0, Steps=5789, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=0, Steps=5808, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=0, Steps=5879, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=0, Steps=5891, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=0, Steps=5902, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=0, Steps=5938, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=0, Steps=5962, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=0, Steps=5978, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=0, Steps=6003, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=0, Steps=6031, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=0, Steps=6042, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=0, Steps=6086, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=0, Steps=6104, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=0, Steps=6125, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=0, Steps=6129, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=0, Steps=6137, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=0, Steps=6170, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=0, Steps=6206, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=0, Steps=6229, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=0, Steps=6239, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=0, Steps=6255, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=0, Steps=6269, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=0, Steps=6274, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=0, Steps=6293, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=0, Steps=6342, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=0, Steps=6407, Training iteration=5
Policy training> Surrogate loss=-0.0031136195175349712, KL divergence=0.006124554201960564, Entropy=1.76466965675354, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.0805799588561058, KL divergence=0.015777364373207092, Entropy=1.740052580833435, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11541267484426498, KL divergence=0.018576569855213165, Entropy=1.7575360536575317, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13007326424121857, KL divergence=0.021557793021202087, Entropy=1.7525571584701538, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.12031495571136475, KL divergence=0.023252973333001137, Entropy=1.7630987167358398, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.12692853808403015, KL divergence=0.025295665487647057, Entropy=1.747620940208435, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.12468592822551727, KL divergence=0.026362217962741852, Entropy=1.7606759071350098, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13754189014434814, KL divergence=0.027823910117149353, Entropy=1.7572038173675537, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13503338396549225, KL divergence=0.027938883751630783, Entropy=1.7567979097366333, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1518050879240036, KL divergence=0.028677362948656082, Entropy=1.7602077722549438, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/96_Step-6407.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/96_Step-6407.ckpt']
Uploaded 3 files for checkpoint 96
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_96.pb
Best checkpoint number: 90, Last checkpoint number: 94
Copying the frozen checkpoint from ./frozen_models/agent/model_90.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'94'}
Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=0, Steps=6421, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=0, Steps=6447, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=0, Steps=6458, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=0, Steps=6480, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=0, Steps=6491, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=0, Steps=6526, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=0, Steps=6568, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=0, Steps=6656, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=0, Steps=6682, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=0, Steps=6703, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=0, Steps=6720, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=0, Steps=6724, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=0, Steps=6771, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=0, Steps=6798, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=0, Steps=6817, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=0, Steps=6843, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=0, Steps=6879, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=0, Steps=6889, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=0, Steps=6890, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=0, Steps=6927, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=0, Steps=6995, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=0, Steps=7012, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=0, Steps=7047, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=0, Steps=7054, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=0, Steps=7088, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=0, Steps=7103, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=0, Steps=7139, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=0, Steps=7161, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=0, Steps=7174, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=0, Steps=7237, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=0, Steps=7289, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=0, Steps=7324, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=0, Steps=7352, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=0, Steps=7362, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=0, Steps=7379, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=0, Steps=7403, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=0, Steps=7442, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=0, Steps=7496, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=0, Steps=7508, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=0, Steps=7553, Training iteration=6
Policy training> Surrogate loss=-0.0009783791610971093, KL divergence=0.0026573454961180687, Entropy=1.8204418420791626, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09627772122621536, KL divergence=0.01560857892036438, Entropy=1.7955228090286255, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.10034208744764328, KL divergence=0.022050410509109497, Entropy=1.7799750566482544, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13001400232315063, KL divergence=0.025129809975624084, Entropy=1.7811553478240967, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1342557668685913, KL divergence=0.026246698573231697, Entropy=1.7920233011245728, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.14344562590122223, KL divergence=0.0271651241928339, Entropy=1.7977174520492554, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1320212185382843, KL divergence=0.02829895168542862, Entropy=1.798049807548523, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13399596512317657, KL divergence=0.028085429221391678, Entropy=1.809821605682373, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13448546826839447, KL divergence=0.029419712722301483, Entropy=1.80925452709198, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.12764574587345123, KL divergence=0.029912404716014862, Entropy=1.8145272731781006, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/97_Step-7553.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/97_Step-7553.ckpt']
Uploaded 3 files for checkpoint 97
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_97.pb
Best checkpoint number: 95, Last checkpoint number: 95
Copying the frozen checkpoint from ./frozen_models/agent/model_95.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'96'}
Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=0, Steps=7568, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=0, Steps=7614, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=0, Steps=7644, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=0, Steps=7665, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=0, Steps=7693, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=0, Steps=7705, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=0, Steps=7775, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=0, Steps=7800, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=0, Steps=7874, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=0, Steps=7894, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=0, Steps=7901, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=0, Steps=7976, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=0, Steps=7996, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=0, Steps=8013, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=0, Steps=8014, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=0, Steps=8039, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=0, Steps=8051, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=0, Steps=8076, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=0, Steps=8081, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=0, Steps=8119, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=0, Steps=8134, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=0, Steps=8147, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=0, Steps=8155, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=0, Steps=8213, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=0, Steps=8229, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=0, Steps=8246, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=0, Steps=8252, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=0, Steps=8288, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=0, Steps=8313, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=0, Steps=8330, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=0, Steps=8387, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=0, Steps=8399, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=0, Steps=8422, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=0, Steps=8430, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=0, Steps=8451, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=0, Steps=8485, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=0, Steps=8568, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=0, Steps=8577, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=0, Steps=8582, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=0, Steps=8604, Training iteration=7
Policy training> Surrogate loss=0.007779255509376526, KL divergence=0.002753960434347391, Entropy=1.802815556526184, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10889425128698349, KL divergence=0.017864268273115158, Entropy=1.756065845489502, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12009064853191376, KL divergence=0.022377338260412216, Entropy=1.7650657892227173, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12152769416570663, KL divergence=0.023923002183437347, Entropy=1.7645320892333984, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.14174170792102814, KL divergence=0.025631383061408997, Entropy=1.7599070072174072, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1390122026205063, KL divergence=0.025111176073551178, Entropy=1.7669613361358643, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1370272934436798, KL divergence=0.025463899597525597, Entropy=1.770685076713562, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.140962615609169, KL divergence=0.02600855939090252, Entropy=1.7708566188812256, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13569685816764832, KL divergence=0.025841280817985535, Entropy=1.7751741409301758, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14419250190258026, KL divergence=0.026426024734973907, Entropy=1.7798980474472046, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/98_Step-8604.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/98_Step-8604.ckpt']
Uploaded 3 files for checkpoint 98
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_98.pb
Best checkpoint number: 95, Last checkpoint number: 96
Copying the frozen checkpoint from ./frozen_models/agent/model_95.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'90'}
Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=0, Steps=8634, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=0, Steps=8653, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=0, Steps=8683, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=0, Steps=8695, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=0, Steps=8708, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=0, Steps=8718, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=0, Steps=8722, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=0, Steps=8761, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=0, Steps=8839, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=0, Steps=8852, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=0, Steps=8873, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=0, Steps=8907, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=0, Steps=8937, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=0, Steps=8976, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=0, Steps=8989, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=0, Steps=9086, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=0, Steps=9107, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=0, Steps=9119, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=0, Steps=9120, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=0, Steps=9177, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=0, Steps=9201, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=0, Steps=9226, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=0, Steps=9242, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=0, Steps=9269, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=0, Steps=9273, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=0, Steps=9311, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=0, Steps=9346, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=0, Steps=9356, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=0, Steps=9397, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=0, Steps=9402, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=0, Steps=9465, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=0, Steps=9501, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=0, Steps=9527, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=0, Steps=9537, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=0, Steps=9551, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=0, Steps=9571, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=0, Steps=9585, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=0, Steps=9608, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=0, Steps=9614, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=0, Steps=9625, Training iteration=8
Policy training> Surrogate loss=0.008384103886783123, KL divergence=0.002280752407386899, Entropy=1.8072383403778076, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.07988797873258591, KL divergence=0.012231564149260521, Entropy=1.8053069114685059, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11624424904584885, KL divergence=0.021301385015249252, Entropy=1.803549885749817, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12403328716754913, KL divergence=0.025055503472685814, Entropy=1.7912133932113647, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1297895312309265, KL divergence=0.025526700541377068, Entropy=1.7924690246582031, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1257750540971756, KL divergence=0.025880541652441025, Entropy=1.7929214239120483, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.12407031655311584, KL divergence=0.02680082991719246, Entropy=1.7963452339172363, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.1253577172756195, KL divergence=0.02740195207297802, Entropy=1.801759123802185, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14575393497943878, KL divergence=0.02801530249416828, Entropy=1.7974103689193726, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.13231466710567474, KL divergence=0.028251688927412033, Entropy=1.798621654510498, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/99_Step-9625.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/99_Step-9625.ckpt']
Uploaded 3 files for checkpoint 99
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_99.pb
Best checkpoint number: 95, Last checkpoint number: 97
Copying the frozen checkpoint from ./frozen_models/agent/model_95.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'90'}
Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=0, Steps=9652, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=0, Steps=9668, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=0, Steps=9699, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=0, Steps=9705, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=0, Steps=9719, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=0, Steps=9744, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=0, Steps=9759, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=0, Steps=9770, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=0, Steps=9784, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=0, Steps=9800, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=0, Steps=9830, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=0, Steps=9871, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=0, Steps=9897, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=0, Steps=9914, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=0, Steps=9921, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=0, Steps=9945, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=0, Steps=10037, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=0, Steps=10097, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=0, Steps=10102, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=0, Steps=10155, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=0, Steps=10192, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=0, Steps=10207, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=0, Steps=10217, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=0, Steps=10244, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=0, Steps=10259, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=0, Steps=10276, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=0, Steps=10285, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=0, Steps=10301, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=0, Steps=10306, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=0, Steps=10466, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=0, Steps=10503, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=0, Steps=10521, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=0, Steps=10538, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=0, Steps=10547, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=0, Steps=10586, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=0, Steps=10611, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=0, Steps=10635, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=0, Steps=10727, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=0, Steps=10728, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=0, Steps=10756, Training iteration=9
Policy training> Surrogate loss=-0.006696254014968872, KL divergence=0.004242672119289637, Entropy=1.7231512069702148, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.0956449955701828, KL divergence=0.025963077321648598, Entropy=1.6645331382751465, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.1146576926112175, KL divergence=0.02681247517466545, Entropy=1.6965335607528687, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13352930545806885, KL divergence=0.02996143512427807, Entropy=1.7049875259399414, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.12527234852313995, KL divergence=0.031034553423523903, Entropy=1.707366943359375, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13859519362449646, KL divergence=0.03236617520451546, Entropy=1.7137713432312012, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13090293109416962, KL divergence=0.03212626650929451, Entropy=1.717829942703247, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13923466205596924, KL divergence=0.032988619059324265, Entropy=1.719944715499878, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13507018983364105, KL divergence=0.03344117850065231, Entropy=1.7170000076293945, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14147382974624634, KL divergence=0.03282201290130615, Entropy=1.7292886972427368, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/100_Step-10756.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/100_Step-10756.ckpt']
Uploaded 3 files for checkpoint 100
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_100.pb
Best checkpoint number: 95, Last checkpoint number: 98
Copying the frozen checkpoint from ./frozen_models/agent/model_95.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'97'}
Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=0, Steps=10785, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=0, Steps=10808, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=0, Steps=10841, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=0, Steps=10857, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=0, Steps=10872, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=0, Steps=10910, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=0, Steps=10927, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=0, Steps=10943, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=0, Steps=11002, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=0, Steps=11014, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=0, Steps=11045, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=0, Steps=11162, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=0, Steps=11176, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=0, Steps=11198, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=0, Steps=11208, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=0, Steps=11244, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=0, Steps=11269, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=0, Steps=11329, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=0, Steps=11338, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=0, Steps=11381, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=0, Steps=11418, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=0, Steps=11436, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=0, Steps=11444, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=0, Steps=11502, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=0, Steps=11540, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=0, Steps=11555, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=0, Steps=11561, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=0, Steps=11593, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=0, Steps=11611, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=0, Steps=11625, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=0, Steps=11682, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=0, Steps=11701, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=0, Steps=11732, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=0, Steps=11763, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=0, Steps=11806, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=0, Steps=11807, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=0, Steps=11834, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=0, Steps=11845, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=0, Steps=11887, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=0, Steps=11916, Training iteration=10
Policy training> Surrogate loss=-5.435198545455933e-06, KL divergence=0.008459006436169147, Entropy=1.692621111869812, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09451238811016083, KL divergence=0.024530623108148575, Entropy=1.680521845817566, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12284442037343979, KL divergence=0.0273338221013546, Entropy=1.6896342039108276, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.1292639523744583, KL divergence=0.03112892061471939, Entropy=1.6846518516540527, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13386647403240204, KL divergence=0.031849298626184464, Entropy=1.7080695629119873, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13758018612861633, KL divergence=0.033985573798418045, Entropy=1.6980005502700806, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.14020471274852753, KL divergence=0.03436414152383804, Entropy=1.710399866104126, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.12827123701572418, KL divergence=0.035895660519599915, Entropy=1.6957972049713135, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14177992939949036, KL divergence=0.035417668521404266, Entropy=1.7062695026397705, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14061608910560608, KL divergence=0.035758499056100845, Entropy=1.701397180557251, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/101_Step-11916.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/101_Step-11916.ckpt']
Uploaded 3 files for checkpoint 101
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_101.pb
Best checkpoint number: 95, Last checkpoint number: 99
Copying the frozen checkpoint from ./frozen_models/agent/model_95.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'98'}
Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=0, Steps=11945, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=0, Steps=11988, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=0, Steps=12019, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=0, Steps=12030, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=0, Steps=12086, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=0, Steps=12103, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=0, Steps=12122, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=0, Steps=12147, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=0, Steps=12204, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=0, Steps=12226, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=0, Steps=12239, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=0, Steps=12270, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=0, Steps=12317, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=0, Steps=12362, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=0, Steps=12421, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=0, Steps=12456, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=0, Steps=12564, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=0, Steps=12587, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=0, Steps=12598, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=0, Steps=12651, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=0, Steps=12673, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=0, Steps=12686, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=0, Steps=12742, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=0, Steps=12748, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=0, Steps=12817, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=0, Steps=12831, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=0, Steps=12848, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=0, Steps=12884, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=0, Steps=12892, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=0, Steps=12901, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=0, Steps=12911, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=0, Steps=12967, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=0, Steps=12989, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=0, Steps=12996, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=0, Steps=13105, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=0, Steps=13131, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=0, Steps=13141, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=0, Steps=13165, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=0, Steps=13171, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=0, Steps=13233, Training iteration=11
Policy training> Surrogate loss=0.0017730150138959289, KL divergence=0.0040632388554513454, Entropy=1.7374992370605469, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09435530751943588, KL divergence=0.023436320945620537, Entropy=1.668975591659546, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12374681234359741, KL divergence=0.024943778291344643, Entropy=1.7077815532684326, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.1285509467124939, KL divergence=0.029758146032691002, Entropy=1.6935443878173828, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13136887550354004, KL divergence=0.03158874437212944, Entropy=1.7040302753448486, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13189683854579926, KL divergence=0.03296791762113571, Entropy=1.7137858867645264, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.14085301756858826, KL divergence=0.03401787579059601, Entropy=1.7085177898406982, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14051467180252075, KL divergence=0.03462429717183113, Entropy=1.717740774154663, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14431950449943542, KL divergence=0.034786444157361984, Entropy=1.7172095775604248, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14557549357414246, KL divergence=0.035425230860710144, Entropy=1.7201812267303467, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/102_Step-13233.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/102_Step-13233.ckpt']
Uploaded 3 files for checkpoint 102
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_102.pb
Best checkpoint number: 100, Last checkpoint number: 100
Copying the frozen checkpoint from ./frozen_models/agent/model_100.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'99'}
Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=0, Steps=13285, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=0, Steps=13342, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=0, Steps=13403, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=0, Steps=13409, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=0, Steps=13420, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=0, Steps=13470, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=0, Steps=13493, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=0, Steps=13505, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=0, Steps=13542, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=0, Steps=13552, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=0, Steps=13600, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=0, Steps=13631, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=0, Steps=13657, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=0, Steps=13664, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=0, Steps=13674, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=0, Steps=13719, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=0, Steps=13729, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=0, Steps=13797, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=0, Steps=13801, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=0, Steps=13851, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=0, Steps=13893, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=0, Steps=13911, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=0, Steps=13936, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=0, Steps=13953, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=0, Steps=13964, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=0, Steps=13980, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=0, Steps=14009, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=0, Steps=14022, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=0, Steps=14042, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=0, Steps=14048, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=0, Steps=14053, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=0, Steps=14072, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=0, Steps=14083, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=0, Steps=14096, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=0, Steps=14103, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=0, Steps=14120, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=0, Steps=14149, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=0, Steps=14175, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=0, Steps=14178, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=0, Steps=14209, Training iteration=12
Policy training> Surrogate loss=0.013156712055206299, KL divergence=0.0020722365006804466, Entropy=1.7388192415237427, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.0843585729598999, KL divergence=0.019638782367110252, Entropy=1.6763471364974976, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11074088513851166, KL divergence=0.02839655615389347, Entropy=1.687774658203125, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12262962758541107, KL divergence=0.03014250472187996, Entropy=1.7014302015304565, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13296273350715637, KL divergence=0.03378380089998245, Entropy=1.6794098615646362, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.12916016578674316, KL divergence=0.03607786074280739, Entropy=1.679194688796997, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1320396512746811, KL divergence=0.0367400124669075, Entropy=1.6802750825881958, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13365036249160767, KL divergence=0.037786055356264114, Entropy=1.6809321641921997, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.1306065171957016, KL divergence=0.03883945196866989, Entropy=1.6795117855072021, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.135799378156662, KL divergence=0.03887150436639786, Entropy=1.6836769580841064, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/103_Step-14209.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/103_Step-14209.ckpt']
Uploaded 3 files for checkpoint 103
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_103.pb
Best checkpoint number: 100, Last checkpoint number: 101
Copying the frozen checkpoint from ./frozen_models/agent/model_100.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'101'}
Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=0, Steps=14258, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=0, Steps=14295, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=0, Steps=14307, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=0, Steps=14338, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=0, Steps=14381, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=0, Steps=14430, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=0, Steps=14490, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=0, Steps=14514, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=0, Steps=14543, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=0, Steps=14559, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=0, Steps=14577, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=0, Steps=14612, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=0, Steps=14647, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=0, Steps=14653, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=0, Steps=14699, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=0, Steps=14733, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=0, Steps=14745, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=0, Steps=14760, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=0, Steps=14763, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=0, Steps=14783, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=0, Steps=14818, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=0, Steps=14851, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=0, Steps=14861, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=0, Steps=14874, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=0, Steps=14906, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=0, Steps=14938, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=0, Steps=14943, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=0, Steps=14960, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=0, Steps=14982, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=0, Steps=15009, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=0, Steps=15019, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=0, Steps=15065, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=0, Steps=15118, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=0, Steps=15132, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=0, Steps=15155, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=0, Steps=15162, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=0, Steps=15186, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=0, Steps=15208, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=0, Steps=15210, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=0, Steps=15248, Training iteration=13
Policy training> Surrogate loss=0.01153283379971981, KL divergence=0.006266102660447359, Entropy=1.7052128314971924, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10127796977758408, KL divergence=0.022570982575416565, Entropy=1.716158390045166, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12191107124090195, KL divergence=0.029372531920671463, Entropy=1.7153304815292358, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12889669835567474, KL divergence=0.033468201756477356, Entropy=1.7091820240020752, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1412992924451828, KL divergence=0.036764852702617645, Entropy=1.7185128927230835, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1365540623664856, KL divergence=0.0384635291993618, Entropy=1.7278480529785156, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1406315565109253, KL divergence=0.039038293063640594, Entropy=1.729247808456421, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14530062675476074, KL divergence=0.03961123898625374, Entropy=1.7283802032470703, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13888506591320038, KL divergence=0.04064130783081055, Entropy=1.7230746746063232, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1370273381471634, KL divergence=0.041022203862667084, Entropy=1.731052041053772, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/104_Step-15248.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/104_Step-15248.ckpt']
Uploaded 3 files for checkpoint 104
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_104.pb
Best checkpoint number: 100, Last checkpoint number: 102
Copying the frozen checkpoint from ./frozen_models/agent/model_100.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'95'}
Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=0, Steps=15291, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=0, Steps=15358, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=0, Steps=15364, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=0, Steps=15383, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=0, Steps=15421, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=0, Steps=15457, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=0, Steps=15483, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=0, Steps=15493, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=0, Steps=15518, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=0, Steps=15545, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=0, Steps=15601, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=0, Steps=15643, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=0, Steps=15659, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=0, Steps=15668, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=0, Steps=15708, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=0, Steps=15729, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=0, Steps=15799, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=0, Steps=15814, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=0, Steps=15825, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=0, Steps=15890, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=0, Steps=15926, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=0, Steps=15968, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=0, Steps=15997, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=0, Steps=16008, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=0, Steps=16016, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=0, Steps=16032, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=0, Steps=16095, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=0, Steps=16128, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=0, Steps=16151, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=0, Steps=16167, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=0, Steps=16187, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=0, Steps=16197, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=0, Steps=16229, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=0, Steps=16237, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=0, Steps=16259, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=0, Steps=16260, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=0, Steps=16271, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=0, Steps=16276, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=0, Steps=16280, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=0, Steps=16320, Training iteration=14
Policy training> Surrogate loss=0.007907703518867493, KL divergence=0.0038944375701248646, Entropy=1.7309393882751465, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10433462262153625, KL divergence=0.021484438329935074, Entropy=1.6804635524749756, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11032484471797943, KL divergence=0.0293326023966074, Entropy=1.6864948272705078, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.11603064835071564, KL divergence=0.03211843967437744, Entropy=1.6774948835372925, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.12802696228027344, KL divergence=0.032889194786548615, Entropy=1.6785603761672974, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1308419406414032, KL divergence=0.03384483605623245, Entropy=1.687279462814331, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1252700835466385, KL divergence=0.03428494185209274, Entropy=1.6936829090118408, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14426696300506592, KL divergence=0.03438033536076546, Entropy=1.7013638019561768, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14411762356758118, KL divergence=0.035872869193553925, Entropy=1.7027361392974854, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1316487193107605, KL divergence=0.036421388387680054, Entropy=1.6963491439819336, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/105_Step-16320.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/105_Step-16320.ckpt']
Uploaded 3 files for checkpoint 105
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_105.pb
Best checkpoint number: 100, Last checkpoint number: 103
Copying the frozen checkpoint from ./frozen_models/agent/model_100.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'102'}
Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=0, Steps=16342, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=0, Steps=16388, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=0, Steps=16427, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=0, Steps=16506, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=0, Steps=16536, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=0, Steps=16544, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=0, Steps=16564, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=0, Steps=16603, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=0, Steps=16614, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=0, Steps=16641, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=0, Steps=16663, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=0, Steps=16693, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=0, Steps=16703, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=0, Steps=16708, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=0, Steps=16725, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=0, Steps=16740, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=0, Steps=16759, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=0, Steps=16776, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=0, Steps=16806, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=0, Steps=16844, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=0, Steps=16881, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=0, Steps=16917, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=0, Steps=16923, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=0, Steps=16958, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=0, Steps=16970, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=0, Steps=16993, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=0, Steps=17014, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=0, Steps=17037, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=0, Steps=17074, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=0, Steps=17086, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=0, Steps=17100, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=0, Steps=17118, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=0, Steps=17139, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=0, Steps=17164, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=0, Steps=17167, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=0, Steps=17195, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=0, Steps=17225, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=0, Steps=17243, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=0, Steps=17282, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=0, Steps=17327, Training iteration=15
Policy training> Surrogate loss=-0.008141460828483105, KL divergence=0.006841917056590319, Entropy=1.6902451515197754, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09834133088588715, KL divergence=0.032979998737573624, Entropy=1.6425453424453735, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11960826814174652, KL divergence=0.03694146126508713, Entropy=1.6666499376296997, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.11223992705345154, KL divergence=0.04010457918047905, Entropy=1.6833552122116089, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1384761482477188, KL divergence=0.042755719274282455, Entropy=1.6923273801803589, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13234491646289825, KL divergence=0.04486389085650444, Entropy=1.6915020942687988, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13117888569831848, KL divergence=0.046617306768894196, Entropy=1.6974297761917114, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14205113053321838, KL divergence=0.047229647636413574, Entropy=1.7010830640792847, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14498086273670197, KL divergence=0.048151906579732895, Entropy=1.6982909440994263, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14455482363700867, KL divergence=0.04943074658513069, Entropy=1.6872738599777222, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/106_Step-17327.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/106_Step-17327.ckpt']
Uploaded 3 files for checkpoint 106
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_106.pb
Best checkpoint number: 104, Last checkpoint number: 104
Copying the frozen checkpoint from ./frozen_models/agent/model_104.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'103'}
Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=0, Steps=17357, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=0, Steps=17405, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=0, Steps=17434, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=0, Steps=17512, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=0, Steps=17527, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=0, Steps=17543, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=0, Steps=17585, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=0, Steps=17599, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=0, Steps=17626, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=0, Steps=17660, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=0, Steps=17698, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=0, Steps=17728, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=0, Steps=17759, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=0, Steps=17780, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=0, Steps=17813, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=0, Steps=17827, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=0, Steps=17838, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=0, Steps=17861, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=0, Steps=17905, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=0, Steps=17956, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=0, Steps=17994, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=0, Steps=18031, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=0, Steps=18057, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=0, Steps=18073, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=0, Steps=18098, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=0, Steps=18121, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=0, Steps=18127, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=0, Steps=18157, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=0, Steps=18174, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=0, Steps=18193, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=0, Steps=18225, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=0, Steps=18241, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=0, Steps=18258, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=0, Steps=18275, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=0, Steps=18282, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=0, Steps=18329, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=0, Steps=18353, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=0, Steps=18371, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=0, Steps=18417, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=0, Steps=18481, Training iteration=16
Policy training> Surrogate loss=-0.0017804040107876062, KL divergence=0.00504349684342742, Entropy=1.673174262046814, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.097516268491745, KL divergence=0.02775431051850319, Entropy=1.6416929960250854, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11402197927236557, KL divergence=0.03061697632074356, Entropy=1.6604111194610596, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12650850415229797, KL divergence=0.03616980463266373, Entropy=1.6530605554580688, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.12891636788845062, KL divergence=0.038740430027246475, Entropy=1.647080659866333, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1344764083623886, KL divergence=0.0403207428753376, Entropy=1.6555203199386597, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13598483800888062, KL divergence=0.04191317409276962, Entropy=1.6603233814239502, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13195864856243134, KL divergence=0.043441396206617355, Entropy=1.6541634798049927, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13410431146621704, KL divergence=0.04298041760921478, Entropy=1.6731435060501099, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.13774056732654572, KL divergence=0.04445186257362366, Entropy=1.6607508659362793, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/107_Step-18481.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/107_Step-18481.ckpt']
Uploaded 3 files for checkpoint 107
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_107.pb
Best checkpoint number: 105, Last checkpoint number: 105
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'100'}
Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=0, Steps=18514, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=0, Steps=18531, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=0, Steps=18592, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=0, Steps=18602, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=0, Steps=18636, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=0, Steps=18658, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=0, Steps=18676, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=0, Steps=18711, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=0, Steps=18724, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=0, Steps=18735, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=0, Steps=18777, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=0, Steps=18782, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=0, Steps=18810, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=0, Steps=18820, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=0, Steps=18846, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=0, Steps=18867, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=0, Steps=18901, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=0, Steps=18929, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=0, Steps=18939, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=0, Steps=19010, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=0, Steps=19076, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=0, Steps=19106, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=0, Steps=19118, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=0, Steps=19183, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=0, Steps=19229, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=0, Steps=19283, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=0, Steps=19305, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=0, Steps=19387, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=0, Steps=19401, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=0, Steps=19420, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=0, Steps=19442, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=0, Steps=19455, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=0, Steps=19504, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=0, Steps=19512, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=0, Steps=19574, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=0, Steps=19580, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=0, Steps=19593, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=0, Steps=19609, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=0, Steps=19610, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=0, Steps=19628, Training iteration=17
Policy training> Surrogate loss=-0.0010097399353981018, KL divergence=0.0053224992007017136, Entropy=1.6565808057785034, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.0867169052362442, KL divergence=0.028900185599923134, Entropy=1.6511412858963013, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12312156707048416, KL divergence=0.04138306528329849, Entropy=1.6358461380004883, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13247016072273254, KL divergence=0.04153871536254883, Entropy=1.6424551010131836, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1348421722650528, KL divergence=0.04296175763010979, Entropy=1.6595691442489624, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13020844757556915, KL divergence=0.04338579252362251, Entropy=1.6545374393463135, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13795609772205353, KL divergence=0.04575471580028534, Entropy=1.6455377340316772, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14040766656398773, KL divergence=0.04628089442849159, Entropy=1.6468021869659424, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14437361061573029, KL divergence=0.047200970351696014, Entropy=1.6483078002929688, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.15211789309978485, KL divergence=0.047641951590776443, Entropy=1.6477999687194824, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/108_Step-19628.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/108_Step-19628.ckpt']
Uploaded 3 files for checkpoint 108
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_108.pb
Best checkpoint number: 105, Last checkpoint number: 106
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'104'}
Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=0, Steps=19644, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=0, Steps=19707, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=0, Steps=19725, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=0, Steps=19751, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=0, Steps=19781, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=0, Steps=19861, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=0, Steps=19878, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=0, Steps=19904, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=0, Steps=19930, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=0, Steps=19937, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=0, Steps=20001, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=0, Steps=20028, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=0, Steps=20045, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=0, Steps=20062, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=0, Steps=20114, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=0, Steps=20126, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=0, Steps=20143, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=0, Steps=20205, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=0, Steps=20249, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=0, Steps=20318, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=0, Steps=20347, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=0, Steps=20387, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=0, Steps=20395, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=0, Steps=20435, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=0, Steps=20452, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=0, Steps=20463, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=0, Steps=20480, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=0, Steps=20488, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=0, Steps=20507, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=0, Steps=20526, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=0, Steps=20598, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=0, Steps=20614, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=0, Steps=20636, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=0, Steps=20659, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=0, Steps=20675, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=0, Steps=20796, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=0, Steps=20861, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=0, Steps=20931, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=0, Steps=20943, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=0, Steps=20981, Training iteration=18
Policy training> Surrogate loss=0.0027198507450520992, KL divergence=0.007113494910299778, Entropy=1.6047435998916626, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10274101793766022, KL divergence=0.03203515708446503, Entropy=1.5678458213806152, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11905329674482346, KL divergence=0.03846067190170288, Entropy=1.55318284034729, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13183584809303284, KL divergence=0.0431508831679821, Entropy=1.549095869064331, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13587813079357147, KL divergence=0.04388975724577904, Entropy=1.5600401163101196, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1411859691143036, KL divergence=0.04722044989466667, Entropy=1.5560368299484253, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.14287149906158447, KL divergence=0.048334456980228424, Entropy=1.5773578882217407, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14643026888370514, KL divergence=0.050604429095983505, Entropy=1.581147313117981, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14274552464485168, KL divergence=0.05185459926724434, Entropy=1.5918978452682495, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1459869146347046, KL divergence=0.052878934890031815, Entropy=1.5924720764160156, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/109_Step-20981.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/109_Step-20981.ckpt']
Uploaded 3 files for checkpoint 109
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_109.pb
Best checkpoint number: 105, Last checkpoint number: 107
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'106'}
Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=0, Steps=21020, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=0, Steps=21050, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=0, Steps=21090, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=0, Steps=21099, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=0, Steps=21133, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=0, Steps=21135, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=0, Steps=21184, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=0, Steps=21204, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=0, Steps=21221, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=0, Steps=21244, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=0, Steps=21264, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=0, Steps=21296, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=0, Steps=21314, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=0, Steps=21319, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=0, Steps=21322, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=0, Steps=21428, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=0, Steps=21497, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=0, Steps=21598, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=0, Steps=21650, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=0, Steps=21707, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=0, Steps=21741, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=0, Steps=21796, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=0, Steps=21813, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=0, Steps=21880, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=0, Steps=21902, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=0, Steps=21953, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=0, Steps=21967, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=0, Steps=21988, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=0, Steps=22003, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=0, Steps=22026, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=0, Steps=22074, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=0, Steps=22102, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=0, Steps=22136, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=0, Steps=22154, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=0, Steps=22158, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=0, Steps=22177, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=0, Steps=22206, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=0, Steps=22214, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=0, Steps=22268, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=0, Steps=22310, Training iteration=19
Policy training> Surrogate loss=0.004233962390571833, KL divergence=0.011185097508132458, Entropy=1.5799742937088013, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.0938325822353363, KL divergence=0.03699883073568344, Entropy=1.573340892791748, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.10216723382472992, KL divergence=0.04209357872605324, Entropy=1.5670875310897827, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12171691656112671, KL divergence=0.04401626065373421, Entropy=1.5842084884643555, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1275709569454193, KL divergence=0.04627951234579086, Entropy=1.5783472061157227, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.12715007364749908, KL divergence=0.046424467116594315, Entropy=1.5904659032821655, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13260087370872498, KL divergence=0.04792017489671707, Entropy=1.5936228036880493, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.1257573366165161, KL divergence=0.05146850273013115, Entropy=1.5723297595977783, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.1383439600467682, KL divergence=0.05089182406663895, Entropy=1.5952099561691284, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.13281486928462982, KL divergence=0.05263661593198776, Entropy=1.5934081077575684, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/110_Step-22310.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/110_Step-22310.ckpt']
Uploaded 3 files for checkpoint 110
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_110.pb
Best checkpoint number: 105, Last checkpoint number: 108
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'107'}
Training> Name=main_level/agent, Worker=0, Episode=801, Total reward=0, Steps=22335, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=802, Total reward=0, Steps=22357, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=803, Total reward=0, Steps=22402, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=804, Total reward=0, Steps=22449, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=805, Total reward=0, Steps=22464, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=806, Total reward=0, Steps=22500, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=807, Total reward=0, Steps=22528, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=808, Total reward=0, Steps=22593, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=809, Total reward=0, Steps=22618, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=810, Total reward=0, Steps=22681, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=811, Total reward=0, Steps=22711, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=812, Total reward=0, Steps=22725, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=813, Total reward=0, Steps=22750, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=814, Total reward=0, Steps=22771, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=815, Total reward=0, Steps=22779, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=816, Total reward=0, Steps=22819, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=817, Total reward=0, Steps=22850, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=818, Total reward=0, Steps=22864, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=819, Total reward=0, Steps=22874, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=820, Total reward=0, Steps=22914, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=821, Total reward=0, Steps=23025, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=822, Total reward=0, Steps=23074, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=823, Total reward=0, Steps=23107, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=824, Total reward=0, Steps=23138, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=825, Total reward=0, Steps=23174, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=826, Total reward=0, Steps=23192, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=827, Total reward=0, Steps=23207, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=828, Total reward=0, Steps=23235, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=829, Total reward=0, Steps=23247, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=830, Total reward=0, Steps=23261, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=831, Total reward=0, Steps=23267, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=832, Total reward=0, Steps=23281, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=833, Total reward=0, Steps=23311, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=834, Total reward=0, Steps=23320, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=835, Total reward=0, Steps=23321, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=836, Total reward=0, Steps=23352, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=837, Total reward=0, Steps=23384, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=838, Total reward=0, Steps=23390, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=839, Total reward=0, Steps=23396, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=840, Total reward=0, Steps=23440, Training iteration=20
Policy training> Surrogate loss=-0.010460247285664082, KL divergence=0.007416403386741877, Entropy=1.5879340171813965, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.0882440060377121, KL divergence=0.04128342494368553, Entropy=1.5380240678787231, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.10957761853933334, KL divergence=0.04883372411131859, Entropy=1.5680363178253174, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.11129046976566315, KL divergence=0.053557660430669785, Entropy=1.564027190208435, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.12766975164413452, KL divergence=0.05450806766748428, Entropy=1.5494645833969116, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.12260384857654572, KL divergence=0.05429871007800102, Entropy=1.559326171875, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.12294577062129974, KL divergence=0.054538577795028687, Entropy=1.5727488994598389, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.12192803621292114, KL divergence=0.054736893624067307, Entropy=1.5843135118484497, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13053740561008453, KL divergence=0.05524304509162903, Entropy=1.5797359943389893, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1325434148311615, KL divergence=0.05449671670794487, Entropy=1.5886809825897217, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/111_Step-23440.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/111_Step-23440.ckpt']
Uploaded 3 files for checkpoint 111
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_111.pb
Best checkpoint number: 105, Last checkpoint number: 109
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'108'}
Training> Name=main_level/agent, Worker=0, Episode=841, Total reward=0, Steps=23470, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=842, Total reward=0, Steps=23494, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=843, Total reward=0, Steps=23564, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=844, Total reward=0, Steps=23579, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=845, Total reward=0, Steps=23586, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=846, Total reward=0, Steps=23605, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=847, Total reward=0, Steps=23619, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=848, Total reward=0, Steps=23656, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=849, Total reward=0, Steps=23682, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=850, Total reward=0, Steps=23694, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=851, Total reward=0, Steps=23708, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=852, Total reward=0, Steps=23759, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=853, Total reward=0, Steps=23771, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=854, Total reward=0, Steps=23797, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=855, Total reward=0, Steps=23798, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=856, Total reward=0, Steps=23843, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=857, Total reward=0, Steps=23876, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=858, Total reward=0, Steps=23894, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=859, Total reward=0, Steps=23936, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=860, Total reward=0, Steps=23985, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=861, Total reward=0, Steps=23996, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=862, Total reward=0, Steps=24026, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=863, Total reward=0, Steps=24053, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=864, Total reward=0, Steps=24079, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=865, Total reward=0, Steps=24086, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=866, Total reward=0, Steps=24099, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=867, Total reward=0, Steps=24115, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=868, Total reward=0, Steps=24147, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=869, Total reward=0, Steps=24174, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=870, Total reward=0, Steps=24175, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=871, Total reward=0, Steps=24198, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=872, Total reward=0, Steps=24231, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=873, Total reward=0, Steps=24261, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=874, Total reward=0, Steps=24279, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=875, Total reward=0, Steps=24287, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=876, Total reward=0, Steps=24293, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=877, Total reward=0, Steps=24315, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=878, Total reward=0, Steps=24332, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=879, Total reward=0, Steps=24342, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=880, Total reward=0, Steps=24384, Training iteration=21
Policy training> Surrogate loss=-0.0004811467952094972, KL divergence=0.004552963189780712, Entropy=1.6312520503997803, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10375960171222687, KL divergence=0.02627686969935894, Entropy=1.6322200298309326, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.1155104711651802, KL divergence=0.03994395583868027, Entropy=1.644740343093872, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12120751291513443, KL divergence=0.0471174456179142, Entropy=1.6101633310317993, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.12523981928825378, KL divergence=0.04917684942483902, Entropy=1.6263633966445923, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1324131041765213, KL divergence=0.05069822072982788, Entropy=1.6271597146987915, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1381591558456421, KL divergence=0.052411265671253204, Entropy=1.622362732887268, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.1276792585849762, KL divergence=0.052588582038879395, Entropy=1.6322740316390991, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14947445690631866, KL divergence=0.05264659970998764, Entropy=1.6391838788986206, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.13071046769618988, KL divergence=0.053014177829027176, Entropy=1.643130898475647, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/112_Step-24384.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/112_Step-24384.ckpt']
Uploaded 3 files for checkpoint 112
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_112.pb
Best checkpoint number: 105, Last checkpoint number: 110
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'109'}
Training> Name=main_level/agent, Worker=0, Episode=881, Total reward=0, Steps=24467, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=882, Total reward=0, Steps=24486, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=883, Total reward=0, Steps=24530, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=884, Total reward=0, Steps=24561, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=885, Total reward=0, Steps=24575, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=886, Total reward=0, Steps=24590, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=887, Total reward=0, Steps=24627, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=888, Total reward=0, Steps=24699, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=889, Total reward=0, Steps=24757, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=890, Total reward=0, Steps=24949, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=891, Total reward=0, Steps=24965, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=892, Total reward=0, Steps=24968, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=893, Total reward=0, Steps=24983, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=894, Total reward=0, Steps=25019, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=895, Total reward=0, Steps=25057, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=896, Total reward=0, Steps=25059, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=897, Total reward=0, Steps=25082, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=898, Total reward=0, Steps=25154, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=899, Total reward=0, Steps=25212, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=900, Total reward=0, Steps=25240, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=901, Total reward=0, Steps=25298, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=902, Total reward=0, Steps=25371, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=903, Total reward=0, Steps=25384, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=904, Total reward=0, Steps=25405, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=905, Total reward=0, Steps=25443, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=906, Total reward=0, Steps=25490, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=907, Total reward=0, Steps=25510, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=908, Total reward=0, Steps=25520, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=909, Total reward=0, Steps=25562, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=910, Total reward=0, Steps=25594, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=911, Total reward=0, Steps=25648, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=912, Total reward=0, Steps=25732, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=913, Total reward=0, Steps=25760, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=914, Total reward=0, Steps=25780, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=915, Total reward=0, Steps=25833, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=916, Total reward=0, Steps=25843, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=917, Total reward=0, Steps=25872, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=918, Total reward=0, Steps=25885, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=919, Total reward=0, Steps=25888, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=920, Total reward=0, Steps=25916, Training iteration=22
Policy training> Surrogate loss=-0.00027054353267885745, KL divergence=0.009967891499400139, Entropy=1.6274584531784058, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10091805458068848, KL divergence=0.03612588718533516, Entropy=1.6126095056533813, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.13092532753944397, KL divergence=0.04239519685506821, Entropy=1.6153972148895264, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13924957811832428, KL divergence=0.045695558190345764, Entropy=1.6231176853179932, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1513432115316391, KL divergence=0.046603161841630936, Entropy=1.6099263429641724, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1471155732870102, KL divergence=0.04812479391694069, Entropy=1.6166043281555176, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1493605673313141, KL divergence=0.05092880502343178, Entropy=1.6179172992706299, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14972224831581116, KL divergence=0.0518733374774456, Entropy=1.6151037216186523, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14992056787014008, KL divergence=0.051420021802186966, Entropy=1.6172181367874146, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14458231627941132, KL divergence=0.052565932273864746, Entropy=1.6225496530532837, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/113_Step-25916.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/113_Step-25916.ckpt']
Uploaded 3 files for checkpoint 113
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_113.pb
Best checkpoint number: 105, Last checkpoint number: 111
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'110'}
Training> Name=main_level/agent, Worker=0, Episode=921, Total reward=0, Steps=25987, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=922, Total reward=0, Steps=26017, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=923, Total reward=0, Steps=26043, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=924, Total reward=0, Steps=26140, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=925, Total reward=0, Steps=26183, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=926, Total reward=0, Steps=26194, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=927, Total reward=0, Steps=26211, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=928, Total reward=0, Steps=26228, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=929, Total reward=0, Steps=26238, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=930, Total reward=0, Steps=26305, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=931, Total reward=0, Steps=26324, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=932, Total reward=0, Steps=26349, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=933, Total reward=0, Steps=26387, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=934, Total reward=0, Steps=26405, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=935, Total reward=0, Steps=26406, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=936, Total reward=0, Steps=26443, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=937, Total reward=0, Steps=26481, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=938, Total reward=0, Steps=26492, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=939, Total reward=0, Steps=26505, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=940, Total reward=0, Steps=26521, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=941, Total reward=0, Steps=26555, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=942, Total reward=0, Steps=26634, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=943, Total reward=0, Steps=26649, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=944, Total reward=0, Steps=26742, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=945, Total reward=0, Steps=26752, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=946, Total reward=0, Steps=26815, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=947, Total reward=0, Steps=26821, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=948, Total reward=0, Steps=26867, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=949, Total reward=0, Steps=26886, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=950, Total reward=0, Steps=26900, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=951, Total reward=0, Steps=26921, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=952, Total reward=0, Steps=26977, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=953, Total reward=0, Steps=27014, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=954, Total reward=0, Steps=27029, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=955, Total reward=0, Steps=27034, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=956, Total reward=0, Steps=27046, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=957, Total reward=0, Steps=27074, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=958, Total reward=0, Steps=27148, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=959, Total reward=0, Steps=27217, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=960, Total reward=0, Steps=27230, Training iteration=23
Policy training> Surrogate loss=0.013773202896118164, KL divergence=0.010456462390720844, Entropy=1.6263341903686523, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09724818170070648, KL divergence=0.032933853566646576, Entropy=1.618659257888794, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12348495423793793, KL divergence=0.042431946843862534, Entropy=1.6054052114486694, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.11900103092193604, KL divergence=0.043624147772789, Entropy=1.6202647686004639, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13986118137836456, KL divergence=0.047425445169210434, Entropy=1.6057285070419312, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.14674149453639984, KL divergence=0.049976103007793427, Entropy=1.6044305562973022, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.14368733763694763, KL divergence=0.051675695925951004, Entropy=1.6080642938613892, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14356425404548645, KL divergence=0.05232700705528259, Entropy=1.6130825281143188, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13727647066116333, KL divergence=0.052388161420822144, Entropy=1.6220932006835938, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14120551943778992, KL divergence=0.05265740677714348, Entropy=1.6266059875488281, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/114_Step-27230.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/114_Step-27230.ckpt']
Uploaded 3 files for checkpoint 114
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_114.pb
Best checkpoint number: 105, Last checkpoint number: 113
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'111'}
Training> Name=main_level/agent, Worker=0, Episode=961, Total reward=0, Steps=27253, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=962, Total reward=0, Steps=27405, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=963, Total reward=0, Steps=27412, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=964, Total reward=0, Steps=27441, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=965, Total reward=0, Steps=27455, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=966, Total reward=0, Steps=27460, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=967, Total reward=0, Steps=27487, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=968, Total reward=0, Steps=27534, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=969, Total reward=0, Steps=27553, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=970, Total reward=0, Steps=27571, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=971, Total reward=0, Steps=27633, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=972, Total reward=0, Steps=27672, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=973, Total reward=0, Steps=27708, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=974, Total reward=0, Steps=27746, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=975, Total reward=0, Steps=27792, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=976, Total reward=0, Steps=27793, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=977, Total reward=0, Steps=27814, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=978, Total reward=0, Steps=27829, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=979, Total reward=0, Steps=27834, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=980, Total reward=0, Steps=28016, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=981, Total reward=0, Steps=28051, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=982, Total reward=0, Steps=28115, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=983, Total reward=0, Steps=28155, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=984, Total reward=0, Steps=28184, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=985, Total reward=0, Steps=28227, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=986, Total reward=0, Steps=28259, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=987, Total reward=0, Steps=28271, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=988, Total reward=0, Steps=28276, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=989, Total reward=0, Steps=28289, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=990, Total reward=0, Steps=28297, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=991, Total reward=0, Steps=28312, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=992, Total reward=0, Steps=28339, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=993, Total reward=0, Steps=28387, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=994, Total reward=0, Steps=28398, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=995, Total reward=0, Steps=28413, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=996, Total reward=0, Steps=28452, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=997, Total reward=0, Steps=28483, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=998, Total reward=0, Steps=28496, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=999, Total reward=0, Steps=28574, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1000, Total reward=0, Steps=28654, Training iteration=24
Policy training> Surrogate loss=-0.0021953710820525885, KL divergence=0.008607511408627033, Entropy=1.6244984865188599, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09561498463153839, KL divergence=0.040449731051921844, Entropy=1.598409652709961, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12430185824632645, KL divergence=0.045684635639190674, Entropy=1.6024550199508667, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12453892827033997, KL divergence=0.0472370870411396, Entropy=1.5882679224014282, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1372329741716385, KL divergence=0.04779992997646332, Entropy=1.5869983434677124, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.14081726968288422, KL divergence=0.04912383481860161, Entropy=1.5908031463623047, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.14163321256637573, KL divergence=0.05091923475265503, Entropy=1.5958918333053589, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14425331354141235, KL divergence=0.05238944664597511, Entropy=1.5937126874923706, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14378243684768677, KL divergence=0.05251403898000717, Entropy=1.6083656549453735, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1451530009508133, KL divergence=0.05362241342663765, Entropy=1.5951732397079468, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/115_Step-28654.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/115_Step-28654.ckpt']
Uploaded 3 files for checkpoint 115
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_115.pb
Best checkpoint number: 105, Last checkpoint number: 113
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'112'}
Training> Name=main_level/agent, Worker=0, Episode=1001, Total reward=0, Steps=28701, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1002, Total reward=0, Steps=28715, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1003, Total reward=0, Steps=28737, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1004, Total reward=0, Steps=28751, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1005, Total reward=0, Steps=28770, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1006, Total reward=0, Steps=28787, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1007, Total reward=0, Steps=28874, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1008, Total reward=0, Steps=28928, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1009, Total reward=0, Steps=28947, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1010, Total reward=0, Steps=28960, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1011, Total reward=0, Steps=28998, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1012, Total reward=0, Steps=29041, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1013, Total reward=0, Steps=29094, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1014, Total reward=0, Steps=29119, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1015, Total reward=0, Steps=29221, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1016, Total reward=0, Steps=29251, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1017, Total reward=0, Steps=29257, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1018, Total reward=0, Steps=29276, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1019, Total reward=0, Steps=29340, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1020, Total reward=0, Steps=29371, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1021, Total reward=0, Steps=29394, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1022, Total reward=0, Steps=29425, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1023, Total reward=0, Steps=29459, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1024, Total reward=0, Steps=29468, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1025, Total reward=0, Steps=29523, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1026, Total reward=0, Steps=29569, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1027, Total reward=0, Steps=29583, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1028, Total reward=0, Steps=29616, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1029, Total reward=0, Steps=29632, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1030, Total reward=0, Steps=29647, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1031, Total reward=0, Steps=29676, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1032, Total reward=0, Steps=29682, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1033, Total reward=0, Steps=29706, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1034, Total reward=0, Steps=29734, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1035, Total reward=0, Steps=29746, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1036, Total reward=0, Steps=29761, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1037, Total reward=0, Steps=29773, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1038, Total reward=0, Steps=29854, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1039, Total reward=0, Steps=29905, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1040, Total reward=0, Steps=29963, Training iteration=25
Policy training> Surrogate loss=0.00888894498348236, KL divergence=0.0062442347407341, Entropy=1.6634397506713867, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09970088303089142, KL divergence=0.033604104071855545, Entropy=1.6538746356964111, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.1207430511713028, KL divergence=0.04231354594230652, Entropy=1.6670129299163818, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13746021687984467, KL divergence=0.04489538073539734, Entropy=1.6562011241912842, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.14134624600410461, KL divergence=0.04674467444419861, Entropy=1.6577160358428955, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1382972151041031, KL divergence=0.04792499542236328, Entropy=1.657031774520874, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1388002187013626, KL divergence=0.04937947541475296, Entropy=1.654146432876587, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13096395134925842, KL divergence=0.05079272389411926, Entropy=1.6569160223007202, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.1420109122991562, KL divergence=0.051500625908374786, Entropy=1.6612764596939087, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.13985438644886017, KL divergence=0.05163118243217468, Entropy=1.666425347328186, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/116_Step-29963.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/116_Step-29963.ckpt']
Uploaded 3 files for checkpoint 116
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_116.pb
Best checkpoint number: 105, Last checkpoint number: 114
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'113'}
Training> Name=main_level/agent, Worker=0, Episode=1041, Total reward=0, Steps=30034, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1042, Total reward=0, Steps=30057, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1043, Total reward=0, Steps=30092, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1044, Total reward=0, Steps=30113, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1045, Total reward=0, Steps=30148, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1046, Total reward=0, Steps=30162, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1047, Total reward=0, Steps=30191, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1048, Total reward=0, Steps=30210, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1049, Total reward=0, Steps=30228, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1050, Total reward=0, Steps=30312, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1051, Total reward=0, Steps=30328, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1052, Total reward=0, Steps=30347, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1053, Total reward=0, Steps=30356, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1054, Total reward=0, Steps=30381, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1055, Total reward=0, Steps=30405, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1056, Total reward=0, Steps=30439, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1057, Total reward=0, Steps=30453, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1058, Total reward=0, Steps=30508, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1059, Total reward=0, Steps=30521, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1060, Total reward=0, Steps=30546, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1061, Total reward=0, Steps=30597, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1062, Total reward=0, Steps=30661, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1063, Total reward=0, Steps=30744, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1064, Total reward=0, Steps=30784, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1065, Total reward=0, Steps=30815, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1066, Total reward=0, Steps=30856, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1067, Total reward=0, Steps=30868, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1068, Total reward=0, Steps=30903, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1069, Total reward=0, Steps=30956, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1070, Total reward=0, Steps=30972, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1071, Total reward=0, Steps=31024, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1072, Total reward=0, Steps=31057, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1073, Total reward=0, Steps=31086, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1074, Total reward=0, Steps=31148, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1075, Total reward=0, Steps=31188, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1076, Total reward=0, Steps=31224, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1077, Total reward=0, Steps=31248, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1078, Total reward=0, Steps=31268, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1079, Total reward=0, Steps=31316, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1080, Total reward=0, Steps=31353, Training iteration=26
Policy training> Surrogate loss=0.015028531663119793, KL divergence=0.008097020909190178, Entropy=1.6868524551391602, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10200496017932892, KL divergence=0.03030514158308506, Entropy=1.6650668382644653, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11962936073541641, KL divergence=0.03746930509805679, Entropy=1.6665312051773071, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12877288460731506, KL divergence=0.0417301245033741, Entropy=1.6542593240737915, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1310884803533554, KL divergence=0.04234691709280014, Entropy=1.660796880722046, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1387006938457489, KL divergence=0.043761976063251495, Entropy=1.6543989181518555, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13357016444206238, KL divergence=0.044630855321884155, Entropy=1.664398431777954, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.15516074001789093, KL divergence=0.04592398926615715, Entropy=1.6593916416168213, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13911893963813782, KL divergence=0.04710567742586136, Entropy=1.6668821573257446, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14560776948928833, KL divergence=0.04807407036423683, Entropy=1.6736863851547241, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/117_Step-31353.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/117_Step-31353.ckpt']
Uploaded 3 files for checkpoint 117
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_117.pb
Best checkpoint number: 105, Last checkpoint number: 115
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'114'}
Training> Name=main_level/agent, Worker=0, Episode=1081, Total reward=0, Steps=31373, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1082, Total reward=0, Steps=31459, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1083, Total reward=0, Steps=31503, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1084, Total reward=0, Steps=31536, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1085, Total reward=0, Steps=31574, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1086, Total reward=0, Steps=31590, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1087, Total reward=0, Steps=31602, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1088, Total reward=0, Steps=31624, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1089, Total reward=0, Steps=31643, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1090, Total reward=0, Steps=31656, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1091, Total reward=0, Steps=31722, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1092, Total reward=0, Steps=31755, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1093, Total reward=0, Steps=31778, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1094, Total reward=0, Steps=31802, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1095, Total reward=0, Steps=31833, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1096, Total reward=0, Steps=31867, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1097, Total reward=0, Steps=31888, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1098, Total reward=0, Steps=31904, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1099, Total reward=0, Steps=31961, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1100, Total reward=0, Steps=32000, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1101, Total reward=0, Steps=32028, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1102, Total reward=0, Steps=32043, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1103, Total reward=0, Steps=32073, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1104, Total reward=0, Steps=32100, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1105, Total reward=0, Steps=32131, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1106, Total reward=0, Steps=32158, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1107, Total reward=0, Steps=32259, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1108, Total reward=0, Steps=32287, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1109, Total reward=0, Steps=32313, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1110, Total reward=0, Steps=32370, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1111, Total reward=0, Steps=32421, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1112, Total reward=0, Steps=32458, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1113, Total reward=0, Steps=32465, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1114, Total reward=0, Steps=32479, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1115, Total reward=0, Steps=32499, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1116, Total reward=0, Steps=32505, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1117, Total reward=0, Steps=32513, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1118, Total reward=0, Steps=32579, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1119, Total reward=0, Steps=32594, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1120, Total reward=0, Steps=32639, Training iteration=27
Policy training> Surrogate loss=0.000865077949129045, KL divergence=0.00824878178536892, Entropy=1.7124220132827759, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09857463091611862, KL divergence=0.03344956785440445, Entropy=1.7112938165664673, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11696598678827286, KL divergence=0.04070248454809189, Entropy=1.6950876712799072, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12422414124011993, KL divergence=0.04239045828580856, Entropy=1.7087608575820923, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13103531301021576, KL divergence=0.04412538558244705, Entropy=1.7023687362670898, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13324382901191711, KL divergence=0.04531917721033096, Entropy=1.699633240699768, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1373005509376526, KL divergence=0.046440962702035904, Entropy=1.6974942684173584, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13426463305950165, KL divergence=0.047052543610334396, Entropy=1.691603422164917, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13030487298965454, KL divergence=0.046385906636714935, Entropy=1.7002270221710205, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.13735929131507874, KL divergence=0.048144999891519547, Entropy=1.7029473781585693, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/118_Step-32639.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/118_Step-32639.ckpt']
Uploaded 3 files for checkpoint 118
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_118.pb
Best checkpoint number: 105, Last checkpoint number: 116
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'115'}
Training> Name=main_level/agent, Worker=0, Episode=1121, Total reward=0, Steps=32667, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1122, Total reward=0, Steps=32699, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1123, Total reward=0, Steps=32720, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1124, Total reward=0, Steps=32730, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1125, Total reward=0, Steps=32753, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1126, Total reward=0, Steps=32789, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1127, Total reward=0, Steps=32798, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1128, Total reward=0, Steps=32829, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1129, Total reward=0, Steps=32858, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1130, Total reward=0, Steps=32880, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1131, Total reward=0, Steps=32932, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1132, Total reward=0, Steps=32943, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1133, Total reward=0, Steps=32973, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1134, Total reward=0, Steps=32994, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1135, Total reward=0, Steps=33025, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1136, Total reward=0, Steps=33043, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1137, Total reward=0, Steps=33055, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1138, Total reward=0, Steps=33063, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1139, Total reward=0, Steps=33118, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1140, Total reward=0, Steps=33142, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1141, Total reward=0, Steps=33195, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1142, Total reward=0, Steps=33224, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1143, Total reward=0, Steps=33240, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1144, Total reward=0, Steps=33329, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1145, Total reward=0, Steps=33345, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1146, Total reward=0, Steps=33416, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1147, Total reward=0, Steps=33444, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1148, Total reward=0, Steps=33451, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1149, Total reward=0, Steps=33478, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1150, Total reward=0, Steps=33527, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1151, Total reward=0, Steps=33584, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1152, Total reward=0, Steps=33599, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1153, Total reward=0, Steps=33668, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1154, Total reward=0, Steps=33689, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1155, Total reward=0, Steps=33690, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1156, Total reward=0, Steps=33697, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1157, Total reward=0, Steps=33721, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1158, Total reward=0, Steps=33740, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1159, Total reward=0, Steps=33792, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1160, Total reward=0, Steps=33817, Training iteration=28
Policy training> Surrogate loss=0.004442772828042507, KL divergence=0.005551351699978113, Entropy=1.7375273704528809, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10073795169591904, KL divergence=0.03284040838479996, Entropy=1.6866040229797363, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12704859673976898, KL divergence=0.0404914990067482, Entropy=1.7032971382141113, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.14144214987754822, KL divergence=0.04293040558695793, Entropy=1.7130991220474243, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13978978991508484, KL divergence=0.04455888643860817, Entropy=1.7164162397384644, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1421649008989334, KL divergence=0.045293889939785004, Entropy=1.7144289016723633, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.14730089902877808, KL divergence=0.04590389505028725, Entropy=1.712669849395752, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14476564526557922, KL divergence=0.0460311621427536, Entropy=1.7238306999206543, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14493431150913239, KL divergence=0.04623144492506981, Entropy=1.7243742942810059, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1449526995420456, KL divergence=0.047242335975170135, Entropy=1.7152020931243896, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/119_Step-33817.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/119_Step-33817.ckpt']
Uploaded 3 files for checkpoint 119
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_119.pb
Best checkpoint number: 105, Last checkpoint number: 117
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'116'}
Training> Name=main_level/agent, Worker=0, Episode=1161, Total reward=0, Steps=33854, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1162, Total reward=0, Steps=33880, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1163, Total reward=0, Steps=33900, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1164, Total reward=0, Steps=33916, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1165, Total reward=0, Steps=33939, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1166, Total reward=0, Steps=33952, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1167, Total reward=0, Steps=33957, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1168, Total reward=0, Steps=33976, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1169, Total reward=0, Steps=33989, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1170, Total reward=0, Steps=34004, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1171, Total reward=0, Steps=34029, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1172, Total reward=0, Steps=34070, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1173, Total reward=0, Steps=34098, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1174, Total reward=0, Steps=34114, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1175, Total reward=0, Steps=34149, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1176, Total reward=0, Steps=34174, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1177, Total reward=0, Steps=34204, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1178, Total reward=0, Steps=34218, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1179, Total reward=0, Steps=34273, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1180, Total reward=0, Steps=34295, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1181, Total reward=0, Steps=34320, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1182, Total reward=0, Steps=34329, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1183, Total reward=0, Steps=34355, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1184, Total reward=0, Steps=34398, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1185, Total reward=0, Steps=34399, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1186, Total reward=0, Steps=34412, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1187, Total reward=0, Steps=34450, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1188, Total reward=0, Steps=34469, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1189, Total reward=0, Steps=34486, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1190, Total reward=0, Steps=34502, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1191, Total reward=0, Steps=34520, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1192, Total reward=0, Steps=34539, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1193, Total reward=0, Steps=34551, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1194, Total reward=0, Steps=34562, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1195, Total reward=0, Steps=34670, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1196, Total reward=0, Steps=34695, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1197, Total reward=0, Steps=34715, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1198, Total reward=0, Steps=34765, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1199, Total reward=0, Steps=34774, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1200, Total reward=0, Steps=34801, Training iteration=29
Policy training> Surrogate loss=0.00464881444349885, KL divergence=0.006373592186719179, Entropy=1.7207798957824707, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09982673078775406, KL divergence=0.030045755207538605, Entropy=1.7135926485061646, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11790761351585388, KL divergence=0.038811784237623215, Entropy=1.7084019184112549, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12349224090576172, KL divergence=0.041673362255096436, Entropy=1.7162445783615112, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.11426350474357605, KL divergence=0.043987762182950974, Entropy=1.7137867212295532, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13052129745483398, KL divergence=0.04557978734374046, Entropy=1.711443543434143, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13132044672966003, KL divergence=0.04569866880774498, Entropy=1.7171560525894165, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.1376924067735672, KL divergence=0.04658886045217514, Entropy=1.7239916324615479, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13120701909065247, KL divergence=0.046707410365343094, Entropy=1.72037935256958, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.137698233127594, KL divergence=0.04743725433945656, Entropy=1.7267041206359863, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/120_Step-34801.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/120_Step-34801.ckpt']
Uploaded 3 files for checkpoint 120
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_120.pb
Best checkpoint number: 105, Last checkpoint number: 118
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'117'}
Training> Name=main_level/agent, Worker=0, Episode=1201, Total reward=0, Steps=34820, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1202, Total reward=0, Steps=34838, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1203, Total reward=0, Steps=34850, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1204, Total reward=0, Steps=34857, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1205, Total reward=0, Steps=34877, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1206, Total reward=0, Steps=34886, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1207, Total reward=0, Steps=34925, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1208, Total reward=0, Steps=34935, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1209, Total reward=0, Steps=34953, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1210, Total reward=0, Steps=34979, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1211, Total reward=0, Steps=34993, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1212, Total reward=0, Steps=35025, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1213, Total reward=0, Steps=35057, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1214, Total reward=0, Steps=35064, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1215, Total reward=0, Steps=35106, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1216, Total reward=0, Steps=35127, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1217, Total reward=0, Steps=35156, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1218, Total reward=0, Steps=35176, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1219, Total reward=0, Steps=35187, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1220, Total reward=0, Steps=35219, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1221, Total reward=0, Steps=35249, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1222, Total reward=0, Steps=35267, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1223, Total reward=0, Steps=35324, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1224, Total reward=0, Steps=35351, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1225, Total reward=0, Steps=35471, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1226, Total reward=0, Steps=35495, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1227, Total reward=0, Steps=35539, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1228, Total reward=0, Steps=35565, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1229, Total reward=0, Steps=35579, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1230, Total reward=0, Steps=35634, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1231, Total reward=0, Steps=35645, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1232, Total reward=0, Steps=35685, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1233, Total reward=0, Steps=35741, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1234, Total reward=0, Steps=35752, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1235, Total reward=0, Steps=35778, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1236, Total reward=0, Steps=35822, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1237, Total reward=0, Steps=35838, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1238, Total reward=0, Steps=35859, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1239, Total reward=0, Steps=35908, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1240, Total reward=0, Steps=35949, Training iteration=30
Policy training> Surrogate loss=-0.0038409680128097534, KL divergence=0.006264089606702328, Entropy=1.7546801567077637, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10183127224445343, KL divergence=0.027859225869178772, Entropy=1.7308380603790283, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.12668344378471375, KL divergence=0.032450973987579346, Entropy=1.7307456731796265, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12994953989982605, KL divergence=0.036965757608413696, Entropy=1.7206754684448242, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.1323494017124176, KL divergence=0.03979972004890442, Entropy=1.7217925786972046, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13016588985919952, KL divergence=0.040218230336904526, Entropy=1.7312946319580078, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.1256416290998459, KL divergence=0.04084445536136627, Entropy=1.7259624004364014, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13681821525096893, KL divergence=0.04255935549736023, Entropy=1.7258731126785278, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14192593097686768, KL divergence=0.04307382553815842, Entropy=1.7303203344345093, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.12719282507896423, KL divergence=0.043050188571214676, Entropy=1.730605125427246, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/121_Step-35949.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/121_Step-35949.ckpt']
Uploaded 3 files for checkpoint 121
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_121.pb
Best checkpoint number: 105, Last checkpoint number: 119
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'118'}
Training> Name=main_level/agent, Worker=0, Episode=1241, Total reward=0, Steps=35963, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1242, Total reward=0, Steps=35983, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1243, Total reward=0, Steps=36017, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1244, Total reward=0, Steps=36041, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1245, Total reward=0, Steps=36077, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1246, Total reward=0, Steps=36127, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1247, Total reward=0, Steps=36140, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1248, Total reward=0, Steps=36168, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1249, Total reward=0, Steps=36212, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1250, Total reward=0, Steps=36226, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1251, Total reward=0, Steps=36315, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1252, Total reward=0, Steps=36341, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1253, Total reward=0, Steps=36363, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1254, Total reward=0, Steps=36405, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1255, Total reward=0, Steps=36470, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1256, Total reward=0, Steps=36500, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1257, Total reward=0, Steps=36522, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1258, Total reward=0, Steps=36541, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1259, Total reward=0, Steps=36553, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1260, Total reward=0, Steps=36623, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1261, Total reward=0, Steps=36657, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1262, Total reward=0, Steps=36679, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1263, Total reward=0, Steps=36703, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1264, Total reward=0, Steps=36735, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1265, Total reward=0, Steps=36745, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1266, Total reward=0, Steps=36760, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1267, Total reward=0, Steps=36792, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1268, Total reward=0, Steps=36814, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1269, Total reward=0, Steps=36822, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1270, Total reward=0, Steps=36840, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1271, Total reward=0, Steps=36877, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1272, Total reward=0, Steps=36894, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1273, Total reward=0, Steps=36927, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1274, Total reward=0, Steps=36944, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1275, Total reward=0, Steps=36970, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1276, Total reward=0, Steps=36981, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1277, Total reward=0, Steps=36999, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1278, Total reward=0, Steps=37019, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1279, Total reward=0, Steps=37081, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1280, Total reward=0, Steps=37132, Training iteration=31
Policy training> Surrogate loss=-0.0010264167794957757, KL divergence=0.00399288022890687, Entropy=1.7384552955627441, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.098122738301754, KL divergence=0.027945591136813164, Entropy=1.7098737955093384, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11854568868875504, KL divergence=0.038620829582214355, Entropy=1.679614782333374, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.1322118192911148, KL divergence=0.040411047637462616, Entropy=1.6943656206130981, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13216637074947357, KL divergence=0.0428401343524456, Entropy=1.6931360960006714, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.14188922941684723, KL divergence=0.044411346316337585, Entropy=1.6970516443252563, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13605482876300812, KL divergence=0.045287828892469406, Entropy=1.6890203952789307, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14725150167942047, KL divergence=0.045494552701711655, Entropy=1.6980777978897095, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13951648771762848, KL divergence=0.04511023312807083, Entropy=1.6968036890029907, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1384853720664978, KL divergence=0.04536913335323334, Entropy=1.708481788635254, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/122_Step-37132.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/122_Step-37132.ckpt']
Uploaded 3 files for checkpoint 122
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_122.pb
Best checkpoint number: 105, Last checkpoint number: 120
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'119'}
Training> Name=main_level/agent, Worker=0, Episode=1281, Total reward=0, Steps=37192, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1282, Total reward=0, Steps=37250, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1283, Total reward=0, Steps=37277, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1284, Total reward=0, Steps=37302, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1285, Total reward=0, Steps=37327, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1286, Total reward=0, Steps=37330, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1287, Total reward=0, Steps=37384, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1288, Total reward=0, Steps=37418, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1289, Total reward=0, Steps=37431, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1290, Total reward=0, Steps=37448, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1291, Total reward=0, Steps=37478, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1292, Total reward=0, Steps=37501, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1293, Total reward=0, Steps=37527, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1294, Total reward=0, Steps=37563, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1295, Total reward=0, Steps=37567, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1296, Total reward=0, Steps=37568, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1297, Total reward=0, Steps=37592, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1298, Total reward=0, Steps=37605, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1299, Total reward=0, Steps=37655, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1300, Total reward=0, Steps=37706, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1301, Total reward=0, Steps=37749, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1302, Total reward=0, Steps=37777, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1303, Total reward=0, Steps=37787, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1304, Total reward=0, Steps=37850, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1305, Total reward=0, Steps=37861, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1306, Total reward=0, Steps=37883, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1307, Total reward=0, Steps=37923, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1308, Total reward=0, Steps=37957, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1309, Total reward=0, Steps=37968, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1310, Total reward=0, Steps=37969, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1311, Total reward=0, Steps=37990, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1312, Total reward=0, Steps=38008, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1313, Total reward=0, Steps=38074, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1314, Total reward=0, Steps=38106, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1315, Total reward=0, Steps=38124, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1316, Total reward=0, Steps=38155, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1317, Total reward=0, Steps=38171, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1318, Total reward=0, Steps=38179, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1319, Total reward=0, Steps=38180, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1320, Total reward=0, Steps=38195, Training iteration=32
Policy training> Surrogate loss=4.336610436439514e-05, KL divergence=0.008721374906599522, Entropy=1.6829969882965088, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09077844768762589, KL divergence=0.03092823550105095, Entropy=1.6754493713378906, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.11989334225654602, KL divergence=0.039990801364183426, Entropy=1.6746516227722168, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.12758561968803406, KL divergence=0.04440827667713165, Entropy=1.6762791872024536, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13996237516403198, KL divergence=0.045306235551834106, Entropy=1.6818451881408691, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13904237747192383, KL divergence=0.04719633236527443, Entropy=1.6783967018127441, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13170230388641357, KL divergence=0.046844400465488434, Entropy=1.6812524795532227, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.13710050284862518, KL divergence=0.04723023250699043, Entropy=1.6958541870117188, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13419872522354126, KL divergence=0.04723108559846878, Entropy=1.6970957517623901, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14938583970069885, KL divergence=0.04705805331468582, Entropy=1.7069759368896484, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/123_Step-38195.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/123_Step-38195.ckpt']
Uploaded 3 files for checkpoint 123
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_123.pb
Best checkpoint number: 105, Last checkpoint number: 121
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'120'}
Training> Name=main_level/agent, Worker=0, Episode=1321, Total reward=0, Steps=38263, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1322, Total reward=0, Steps=38283, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1323, Total reward=0, Steps=38320, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1324, Total reward=0, Steps=38326, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1325, Total reward=0, Steps=38327, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1326, Total reward=0, Steps=38339, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1327, Total reward=0, Steps=38372, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1328, Total reward=0, Steps=38418, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1329, Total reward=0, Steps=38434, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1330, Total reward=0, Steps=38447, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1331, Total reward=0, Steps=38465, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1332, Total reward=0, Steps=38481, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1333, Total reward=0, Steps=38525, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1334, Total reward=0, Steps=38555, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1335, Total reward=0, Steps=38574, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1336, Total reward=0, Steps=38575, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1337, Total reward=0, Steps=38603, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1338, Total reward=0, Steps=38613, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1339, Total reward=0, Steps=38614, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1340, Total reward=0, Steps=38652, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1341, Total reward=0, Steps=38681, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1342, Total reward=0, Steps=38710, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1343, Total reward=0, Steps=38728, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1344, Total reward=0, Steps=38763, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1345, Total reward=0, Steps=38766, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1346, Total reward=0, Steps=38808, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1347, Total reward=0, Steps=38842, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1348, Total reward=0, Steps=38856, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1349, Total reward=0, Steps=38907, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1350, Total reward=0, Steps=38919, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1351, Total reward=0, Steps=38961, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1352, Total reward=0, Steps=38979, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1353, Total reward=0, Steps=38994, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1354, Total reward=0, Steps=39061, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1355, Total reward=0, Steps=39062, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1356, Total reward=0, Steps=39078, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1357, Total reward=0, Steps=39083, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1358, Total reward=0, Steps=39150, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1359, Total reward=0, Steps=39152, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1360, Total reward=0, Steps=39243, Training iteration=33
Policy training> Surrogate loss=0.006313745863735676, KL divergence=0.005572695285081863, Entropy=1.7245186567306519, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.09306485205888748, KL divergence=0.032349031418561935, Entropy=1.7013791799545288, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.13014473021030426, KL divergence=0.038403868675231934, Entropy=1.7186055183410645, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.13990388810634613, KL divergence=0.04197240248322487, Entropy=1.7069308757781982, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.14617091417312622, KL divergence=0.04256001114845276, Entropy=1.7276318073272705, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.13916684687137604, KL divergence=0.04482083395123482, Entropy=1.7292871475219727, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.14143943786621094, KL divergence=0.0460406132042408, Entropy=1.7270286083221436, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.14523546397686005, KL divergence=0.045741427689790726, Entropy=1.7352625131607056, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.15560775995254517, KL divergence=0.046651020646095276, Entropy=1.7393486499786377, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14186327159404755, KL divergence=0.046896886080503464, Entropy=1.7397396564483643, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/124_Step-39243.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/124_Step-39243.ckpt']
Uploaded 3 files for checkpoint 124
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_124.pb
Best checkpoint number: 105, Last checkpoint number: 122
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'121'}
Training> Name=main_level/agent, Worker=0, Episode=1361, Total reward=0, Steps=39281, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1362, Total reward=0, Steps=39294, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1363, Total reward=0, Steps=39314, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1364, Total reward=0, Steps=39353, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1365, Total reward=0, Steps=39376, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1366, Total reward=0, Steps=39419, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1367, Total reward=0, Steps=39435, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1368, Total reward=0, Steps=39481, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1369, Total reward=0, Steps=39527, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1370, Total reward=0, Steps=39535, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1371, Total reward=0, Steps=39604, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1372, Total reward=0, Steps=39655, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1373, Total reward=0, Steps=39695, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1374, Total reward=0, Steps=39713, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1375, Total reward=0, Steps=39745, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1376, Total reward=0, Steps=39782, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1377, Total reward=0, Steps=39804, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1378, Total reward=0, Steps=39866, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1379, Total reward=0, Steps=39877, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1380, Total reward=0, Steps=39923, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1381, Total reward=0, Steps=39936, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1382, Total reward=0, Steps=39964, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1383, Total reward=0, Steps=39968, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1384, Total reward=0, Steps=39996, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1385, Total reward=0, Steps=40019, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1386, Total reward=0, Steps=40052, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1387, Total reward=0, Steps=40069, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1388, Total reward=0, Steps=40120, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1389, Total reward=0, Steps=40129, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1390, Total reward=0, Steps=40134, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1391, Total reward=0, Steps=40155, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1392, Total reward=0, Steps=40175, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1393, Total reward=0, Steps=40210, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1394, Total reward=0, Steps=40263, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1395, Total reward=0, Steps=40314, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1396, Total reward=0, Steps=40355, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1397, Total reward=0, Steps=40372, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1398, Total reward=0, Steps=40446, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1399, Total reward=0, Steps=40536, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1400, Total reward=0, Steps=40604, Training iteration=34
Policy training> Surrogate loss=0.0065861293114721775, KL divergence=0.008615043945610523, Entropy=1.7351444959640503, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.10049295425415039, KL divergence=0.030448399484157562, Entropy=1.7042230367660522, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.124122254550457, KL divergence=0.0353231243789196, Entropy=1.7088987827301025, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.1294260323047638, KL divergence=0.03808652237057686, Entropy=1.7023028135299683, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.13684482872486115, KL divergence=0.039387673139572144, Entropy=1.7093749046325684, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.1331888884305954, KL divergence=0.03949500992894173, Entropy=1.7141550779342651, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.13856057822704315, KL divergence=0.040227100253105164, Entropy=1.7142671346664429, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.1372021734714508, KL divergence=0.040867991745471954, Entropy=1.7148946523666382, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.13396131992340088, KL divergence=0.041167356073856354, Entropy=1.7203829288482666, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.14455291628837585, KL divergence=0.04133027046918869, Entropy=1.7211030721664429, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/125_Step-40604.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/125_Step-40604.ckpt']
Uploaded 3 files for checkpoint 125
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_125.pb
Best checkpoint number: 105, Last checkpoint number: 123
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'122'}
Training> Name=main_level/agent, Worker=0, Episode=1401, Total reward=0, Steps=40619, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1402, Total reward=0, Steps=40647, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1403, Total reward=0, Steps=40663, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1404, Total reward=0, Steps=40673, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1405, Total reward=0, Steps=40692, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1406, Total reward=0, Steps=40700, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1407, Total reward=0, Steps=40707, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1408, Total reward=0, Steps=40782, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1409, Total reward=0, Steps=40802, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1410, Total reward=0, Steps=40815, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1411, Total reward=0, Steps=40839, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1412, Total reward=0, Steps=40892, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1413, Total reward=0, Steps=40935, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1414, Total reward=0, Steps=40950, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1415, Total reward=0, Steps=40966, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1416, Total reward=0, Steps=40972, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1417, Total reward=0, Steps=41004, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1418, Total reward=0, Steps=41023, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1419, Total reward=0, Steps=41027, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1420, Total reward=0, Steps=41058, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1421, Total reward=0, Steps=41085, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1422, Total reward=0, Steps=41115, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1423, Total reward=0, Steps=41126, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1424, Total reward=0, Steps=41207, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1425, Total reward=0, Steps=41248, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1426, Total reward=0, Steps=41263, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1427, Total reward=0, Steps=41281, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1428, Total reward=0, Steps=41311, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1429, Total reward=0, Steps=41327, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1430, Total reward=0, Steps=41341, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1431, Total reward=0, Steps=41359, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1432, Total reward=0, Steps=41382, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1433, Total reward=0, Steps=41446, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1434, Total reward=0, Steps=41456, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1435, Total reward=0, Steps=41477, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1436, Total reward=0, Steps=41482, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1437, Total reward=0, Steps=41509, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1438, Total reward=0, Steps=41520, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1439, Total reward=0, Steps=41529, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1440, Total reward=0, Steps=41597, Training iteration=35
Policy training> Surrogate loss=0.0045124683529138565, KL divergence=0.0038171119522303343, Entropy=1.7457515001296997, training epoch=0, learning_rate=0.0001
Policy training> Surrogate loss=-0.1078309714794159, KL divergence=0.02546393685042858, Entropy=1.6931294202804565, training epoch=1, learning_rate=0.0001
Policy training> Surrogate loss=-0.1280236542224884, KL divergence=0.03469649702310562, Entropy=1.693731665611267, training epoch=2, learning_rate=0.0001
Policy training> Surrogate loss=-0.14484208822250366, KL divergence=0.038439393043518066, Entropy=1.7161850929260254, training epoch=3, learning_rate=0.0001
Policy training> Surrogate loss=-0.14163866639137268, KL divergence=0.041216034442186356, Entropy=1.7113579511642456, training epoch=4, learning_rate=0.0001
Policy training> Surrogate loss=-0.14674195647239685, KL divergence=0.04296219348907471, Entropy=1.7114030122756958, training epoch=5, learning_rate=0.0001
Policy training> Surrogate loss=-0.15081213414669037, KL divergence=0.043370865285396576, Entropy=1.7165069580078125, training epoch=6, learning_rate=0.0001
Policy training> Surrogate loss=-0.1450306475162506, KL divergence=0.04415791481733322, Entropy=1.7240201234817505, training epoch=7, learning_rate=0.0001
Policy training> Surrogate loss=-0.14350320398807526, KL divergence=0.044755756855010986, Entropy=1.7303948402404785, training epoch=8, learning_rate=0.0001
Policy training> Surrogate loss=-0.1453123688697815, KL divergence=0.043704163283109665, Entropy=1.7404990196228027, training epoch=9, learning_rate=0.0001
INFO:tensorflow:./checkpoint/126_Step-41597.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/126_Step-41597.ckpt']
Uploaded 3 files for checkpoint 126
INFO:tensorflow:Froze 15 variables.
INFO:tensorflow:Converted 15 variables to const ops.
saved intermediate frozen graph: data-6726c8b5-409a-4089-b607-cb81381f3e2d/models/Evo-TT-center-line-oa-2/sagemaker-robomaker-artifacts/model/model_126.pb
Best checkpoint number: 105, Last checkpoint number: 124
Copying the frozen checkpoint from ./frozen_models/agent/model_105.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'123'}
Training> Name=main_level/agent, Worker=0, Episode=1441, Total reward=0, Steps=41681, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1442, Total reward=0, Steps=41713, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1443, Total reward=0, Steps=41720, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1444, Total reward=0, Steps=41742, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1445, Total reward=0, Steps=41783, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1446, Total reward=0, Steps=41805, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1447, Total reward=0, Steps=41822, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1448, Total reward=0, Steps=41838, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1449, Total reward=0, Steps=41864, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1450, Total reward=0, Steps=41883, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1451, Total reward=0, Steps=41919, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1452, Total reward=0, Steps=41956, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1453, Total reward=0, Steps=41978, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1454, Total reward=0, Steps=41995, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1455, Total reward=0, Steps=42029, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1456, Total reward=0, Steps=42030, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1457, Total reward=0, Steps=42048, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1458, Total reward=0, Steps=42059, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1459, Total reward=0, Steps=42175, Training iteration=36
